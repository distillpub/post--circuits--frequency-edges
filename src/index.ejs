<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1><span style="display: inline; color: gray; margin-right: 0.25em">2.4</span>High/Low Frequency Detectors</h1>
    <p>A family of early-vision filters reacting to contrasts between spatial gratings of different frequency</p>
    <figure class="l-body todo">I'm just a figure who's a hero for fun.</figure>
  </d-title>

  <d-article>

    <h2 style="display: none;">Introduction</h2>
    <p>
      Why should you study awkwardly-named "high/low frequency detectors" when we've just walked you through curve
      detectors in detail? There's a strategic argument from the author's side: everybodyÂ® would guess that vision
      networks contain curve detectors. It's easy to imagine how they'are built up from earlier edge detectors, and
      they're common enough that neuroscientists routinely find them in animal visual corti. "High/low frequency
      detectors", on the other hand, form a family of features we did not expect to find. Still, they have many of the
      hallmarks of important features: their existence feels natural in retrospect, they're an important part of later,
      higher-level features, and they're common across network architectures.
    </p>


    <!--Empirical observation-->
    <h2>Empirical Observation</h2>

    <h4>Feature Visualization</h4>
    <p>We originally found high/low frequency detectors by studying models' neurons' <a
        href="//distill.pub/2017/feature-visualization">feature visualizations</a>: synthetic inputs
      optimized to elicit maximal activation of a single, specific neuron.</p>
    <style>
      .gallery {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(28px, 56px));
        grid-gap: 0.5rem;
        justify-content: start;
      }

      ul.gallery {
        padding-left: 0;
      }

      .gallery img {
        max-width: 100%;
        width: unset;
        object-fit: none;
        object-position: center;
        border-radius: 0.25rem;
      }

      @media screen and (min-width: 768px) {
        .gallery {
          grid-template-columns: repeat(7, minmax(28px, 112px));
          grid-gap: 1rem;
          justify-content: center;
        }
      }
    </style>
    <figure class="l-screen base-grid" id="1.1-feature-vis">
      <ul class="gallery l-page">
        <% for (const unit of require("../static/diagrams/1.1-feature-vis/units.json").units) {%>
        <img src=<%= `diagrams/1.1-feature-vis/neuron-${unit}.png` %> />
        <% } %>
      </ul>
      <figcaption class="figcaption l-body">A variety of detectors from InceptionV1's <code>mixed3a</code> layer.
      </figcaption>
    </figure>

    <h4>Dataset Examples</h4>
    <p>
      To ensure we're not misreading those feature visualizations it's good practice to check what such filters
      maximally activate on over a natural data distribution, say the training set.
    </p>
    <figure id="1.2-dataset-examples">
      <img src="diagrams/1.2-dataset-examples/placeholder.png" />
      <figcaption class="figcaption">
        Crops taken from training dataset, Imagenet, on which a High-Low Frequency detector activated maximally;
        argmaxed over spatial locations.
      </figcaption>
    </figure>

    <p>
      The dataset examples show that a wide range of real-world objects can cause these detectors to fire: deep 3D
      textures, such as on the microphone against a blurred background, but also surface textures such as the MP3
      player's metal finish against it's shiny screen.
    </p>

    <h4>Tuning Curves</h4>
    <p>
      In the curve detector chapter we saw an example of a tuning curve: it plots filter response over rotation angle,
      where the input is a natural image that we know has curvature in it. We can repeat this experiment for high/low
      frequency detectors: since their response is also specific to an angle, we expect to see a similar pattern of
      sensitivity. (And we do.)
    </p>

    <figure id="1.3-1D-tuning-curve">
      <img src="diagrams/1.3-1D-tuning-curve/placeholder.png" />
      <figcaption class="figcaption">
        TODO: replace with 1D tuning curve over angles
      </figcaption>
    </figure>

    <p>
      But high/low frequency detectors seem to have more knobs to turn than just angle: the frequencies of the two
      gratings that make up the frequency edge should also affect the filters' responses. Let's measure activations over
      a space that varies both of those, as well as orientation: Angle by frequency 1 by frequency 2. We will then have
      to visualize this volume in various ways to show it on this 2D screen.
    </p>

    <figure id="1.4-tuning-curves">
      <img src="diagrams/1.4-tuning-curves/placeholder.png" />
      <figcaption class="figcaption">
        TODO: replace with slices of 3D tuning curve over angles, frequency 1, frequency 2?
      </figcaption>
    </figure>

    <p>
      By argmaxing over the two frequencies we recover the angle-response curve from the previous section:
    </p>


    <!--Upstream Circuits-->
    <h2>Upstream Circuits</h2>
    <p>How are High-Low frequency detectors constructed from lower-level features?</p>

    <h4>Circuit diagram</h4>

    <p class="todo">Has linearization been explained before?</p>

    <p>To understand how these detectors are implemented in terms of even earlier features, we can look at which
      features feed into a given high/low frequency detector, and even at the spatial structure of the weights
      connecting those features to the detector in question.</p>

    <figure id="2.1-upstream-circuit">
      <img src="diagrams/3.1-downstream-circuit/placeholder.png" />
      <figcaption class="figcaption">
        TODO: replace with Nick's interactive version on integration?
      </figcaption>
    </figure>

    <h4>Joint NMF of weights</h4>
    <p>Separately?</p>

    <figure id="2.2-upstream-nmf">
      <img src="diagrams/2.2-upstream-nmf/placeholder.png" />
      <figcaption class="figcaption">
        TODO: how to slice, how to present?
      </figcaption>
    </figure>


    <!--Downstream Circuits-->
    <h2>Downstream Circuits</h2>
    <p>How are High-Low frequency detectors used in the construction of higher-level features?</p>

    <figure id="3.1-downstream-circuit">
      <img src="diagrams/3.1-downstream-circuit/placeholder.png" />
      <figcaption class="figcaption">
        TODO: replace with Nick's interactive version on integration?
      </figcaption>
    </figure>

    <p>Object boundary detectors, and how they're dissimilar from edge detectors/their invariances</p>
    <p>Show how opposing high/low frequency detectors are used to make boundary detectors invariant to orientiation</p>

    <figure id="3.2-downstream-nmf">
      <img src="diagrams/3.2-downstream-nmf/placeholder.png" />
      <figcaption class="figcaption">
        TODO: Maybe have better ideas after doing 2.2 NMF diagram
      </figcaption>
    </figure>

    <p class="todo">Maybe use cosine similarity overlay to show how object boundaries get detected by such detectors.
    </p>


    <!--Universality-->
    <h2>Universality</h2>
    <p>How common are High-Low frequency detectors in convolutional neural networks?</p>
    <p>Always good to ask if what you see is the rule or an interesting exception. high/low frequency detectors seem to
      be the rule.</p>

    <h4>Expressability in other models' filter basis</h4>

    <figure id="4.1-similar-directions">
      <img src="diagrams/4.1-similar-directions/placeholder.jpg" />
      <figcaption class="figcaption">
        Linear approximations of similar filters in other models
      </figcaption>
    </figure>

    <h4>High/Low frequency detectors in other models</h4>
    <p>Show grid of similar directions in other models.</p>

    <figure id="4.2-similar-detectors">
      <img src="diagrams/4.2-similar-detectors/placeholder-1.jpg" />
      <img src="diagrams/4.2-similar-detectors/placeholder-2.jpg" />
      <figcaption class="figcaption">
        Top k filters responding to stimuli from earlier section
      </figcaption>
    </figure>

    <h4>Re-occurence at deeper layers at different scale</h4>
    <p>Show re-occurence at higher layers at different scales?</p>

    <figure id="4.3-higher-layers">
      <img src="diagrams/4.3-higher-layers/placeholder.png" />
      <figcaption class="figcaption">
        Top k filters responding to stimuli from earlier section
      </figcaption>
    </figure>


    <h2>Conclusion</h2>
    <p>?</p>

  </d-article>



  <d-appendix>
    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p>

    <!--Potential Limitations-->
    <h3>Limitations</h3>
    <h4>How bad is the linearization?</h4>
    <p class="todo">Maybe we can measure and put up some <em>summary statistics</em> on how bad our linear approximation
      really is.
      Maybe in terms of % variance of resulting activation? -> colah if there's sth more principled.</p>

    <figure id="5.1-linearization-badness">
      <img src="diagrams/5.1-linearization-badness/placeholder.jpg" />
      <figcaption class="figcaption">
        Maybe show % of variance explained over distance between layers or sth; if so likely move to appendix.
      </figcaption>
    </figure>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>