<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>
  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <style>
  .header-self-link {
    border-bottom: none;
  }
  .header-self-link:hover {
    border-bottom: none;
  }
  </style>

  <d-title>
    <h1>High-Low Frequency Detectors</h1>
    <p>A family of early-vision neurons reacting to directional transitions from high to low spatial frequency.</p>
    <figure id="hero" class="l-body">
      <img src="images/high-low-hero.png"/>
    </figure>
  </d-title>

  <d-article>

    <h2 style="display: none;">Introduction</h2>

    <p>Some of the neurons in vision models are features that we aren't particularly surprised to find. <a href="https://distill.pub/2020/circuits/curve-detectors/">Curve detectors</a>, for example, are a pretty natural feature for a vision system to have. In fact, they had already been discovered in the animal visual cortex<d-cite bibtex-key="tang2018complex,pasupathy2001shape,jiang2019discrete"></d-cite>. It's easy to imagine how curve detectors are built up from earlier edge detectors, and it's easy to guess why curve detection might be useful to the rest of the neural network.</p>

    <p>High-low frequency detectors, on the other hand, seem more surprising. They are not a feature that we would have expected <i>a priori</i> to find. Yet, when systematically characterizing<d-cite bibtex-key="olah2020an"></d-cite> the <a href="https://distill.pub/2020/circuits/early-vision/">early layers</a> of InceptionV1<d-cite bibtex-key="szegedy2015going"></d-cite>, we found a full <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_high_low_frequency">fifteen neurons</a> of <code>mixed3a</code> that appear to detect a high frequency pattern on one side, and a low frequency pattern on the other.</p>

    <p>
      One worry we might have about the <a href="https://distill.pub/2020/circuits/zoom-in/">circuits</a> approach<d-cite bibtex-key="olah2020zoom"></d-cite> to studying neural networks is that we might only be able to understand a limited set of highly-intuitive features.

      High-low frequency detectors demonstrate that it's possible to understand at least somewhat unintuitive features.
    </p>

    <a class="header-self-link" href="#function">
      <h2 id="function">
        Function
      </h2>
    </a>

    <p>
      How can we be sure that &ldquo;high-low frequency detectors&rdquo; are actually detecting directional transitions from low to high spatial frequency?
      We will rely on three methods:
    </p>
    <ul>
      <li>
        <a href="#feature-visualization"><b>Feature visualization</b></a> allows us to establish a causal link between each neuron and its function<d-cite bibtex-key="erhan2009visualizing,zeiler2014visualizing,yosinski2015understanding,karpathy2015visualizing,olah2017feature"></d-cite>.
      </li>
      <li>
        <a href="#dataset-examples"><b>Dataset examples</b></a> show us where the neuron fires in practice.
      </li>
      <li>
        <a href="#tuning-curves"><b>Synthetic tuning curves</b></a> show us how variation affects the neuron's response.
      </li>
    </ul>

    <p>
      Later on in the article, we dive into the mechanistic details of how they are both <a href="#implementation">implemented</a> and <a href="#usage">used</a>. We will be able to understand the algorithm that implements them, confirming that they detect high to low frequency transitions.
    </p>

    <a class="header-self-link" href="#feature-visualization">
      <h3 id="feature-visualization">Feature Visualization</h3>
    </a>

    <p>
      A <a href="https://distill.pub/2017/feature-visualization">feature visualization</a><d-cite bibtex-key="erhan2009visualizing,zeiler2014visualizing,yosinski2015understanding,olah2017feature"></d-cite> is a synthetic input
      optimized to elicit maximal activation of a single, specific neuron.
      Feature visualizations are constructed starting from random noise, so each and every pixel in a feature visualization
      that's <i>changed</i> from random noise is there because it caused the neuron to activate more strongly. This
      establishes a causal link! The behavior shown in the
      feature visualization is behavior that causes the neuron to fire:
    </p>

    <style>
      .gallery {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(28px, 64px));
        grid-gap: 0.5rem;
        justify-content: start;
      }

      ul.gallery {
        padding-left: 0;
      }

      .gallery img,
      .gallery-img {
        max-width: 100%;
        width: unset;
        object-fit: none;
        object-position: center;
        border-radius: 8px;
      }

      @media screen and (min-width: 768px) {
        .gallery {
          grid-template-columns: repeat(7, minmax(28px, 96px));
          justify-content: left;
        }
      }
      @media screen and (min-width: 1180px) {
        .gallery {
          grid-gap: 1rem;
        }
      }
    </style>
    <figure id="figure-1">
      <ul class="gallery l-page">
        <% for (const unit of require("../static/diagrams/1.1-feature-vis/units.json").units) {%>
        <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_${unit}.html" style="border-bottom: none; display: flex; flex-direction: column;">
          <img src="<%= `diagrams/1.1-feature-vis/neuron-${unit}.png` %>" title="<%= "Unit " + unit %>" />
          <span class="figcaption"><%= `3a:${unit}`%></span>
        </a>
        <% } %>
      </ul>
      <figcaption class="figcaption l-body">
        <a href="#figure-1" class="figure-number">1</a>:
        Feature visualizations of a variety of high-low frequency detectors from InceptionV1's <a href="https://microscope.openai.com/models/inceptionv1/mixed3a_0?models.op.feature_vis.type=neuron&models.op.technique=feature_vis"><code>mixed3a</code></a> layer.
      </figcaption>
    </figure>

    <p>
      From their feature visualizations, we observe that all of these high-low frequency detectors share these same
      characteristics:
    </p>

    <ul>
      <li>
        <b>Detection of adjacent high and low frequencies.</b> The detectors respond to <i>high frequency</i> on one side, and <i>low frequency</i> on the other side.
      </li>
      <!-- <li>
        <b>Orientedness.</b> The detectors detect their high-low frequency change along a particular angle.
      </li> -->
      <li>
        <b>Rotational equivariance.</b>
        The detectors are rotationally <a href="https://distill.pub/2020/circuits/equivariance/">equivariant</a>: each unit detects a high-low frequency change along a particular angle, with different units spanning the full 360º of possible orientations.
        We will see this in more detail when we <a href="#tuning-curves">construct a tuning curve</a> with synthetic examples, and also when we look at the weights <a href="#implementation">implementing</a> these detectors.
      </li>
    </ul>
    <aside>
      <p>By &ldquo;high frequency&rdquo; and &ldquo;low frequency&rdquo; here, we mean <a
          href="https://en.wikipedia.org/wiki/Spatial_frequency">spatial frequency</a> &mdash; just like when we take the
        Fourier transform of an image.</p>
    </aside>

    <p>We can use a <a href="https://distill.pub/2017/feature-visualization/#diversity">diversity term</a> in our feature visualizations to jointly optimize for the activation of a neuron while encouraging different activation patterns in a batch of visualizations.

    We are thus reasonably confident that if high-low frequency detectors were also sensitive to other patterns, we would see signs of them in these feature visualizations. Instead, the frequency contrast remains an invariant aspect of all these visualizations. (Although other patterns form along the boundary, these are likely outside the neuron's effective receptive field.)</p>

    <figure id="figure-1-2">
      <div class="gallery l-page" style="margin-bottom: 1em;">
        <% for (const unit of [0,1,2,3,4,5,6]) {%>
        <img src="<%= `diagrams/1.1-feature-vis/fv-mixed3a-136-diversity-${unit}.png` %>" title="<%= "Unit mixed3a:136 optimized with a diversity objective."%>" />
        <% } %>
        </div>
      <figcaption class="figcaption l-body">
        <a href="#figure-1-2" class="figure-number">1-2</a>:
        Feature visualizations of high-low frequency detector mixed3a:136 from InceptionV1's <a
          href="https://microscope.openai.com/models/inceptionv1/mixed3a_0?models.op.feature_vis.type=neuron&models.op.technique=feature_vis"><code>mixed3a</code></a>
        layer, optimized with a diversity objective. You can learn more about feature visualization and the diversity objective <a href="https://distill.pub/2017/feature-visualization/#diversity">here</a>.
      </figcaption>
    </figure>

    <a href="#dataset-examples" class="header-self-link">
      <h3 id="dataset-examples">Dataset Examples</h3>
    </a>
    <p>
      We generate dataset examples by sampling from a natural data distribution (in this case, the training set) and selecting the images that cause the neurons to maximally activate.


      Checking against these examples helps ensure we're not misreading the feature visualizations.

    </p>

    <figure id="1.2.0-dataset-examples">
      <div style="display: grid; grid-template-columns: min-content 5fr 1fr; margin-bottom: 1em;">
        <a class="undecorated" href="https://microscope.openai.com/models/inceptionv1/mixed3a_0/136" style="margin-right: 1em;">
          <img src="diagrams/1.1-feature-vis/neuron-136.png" class="gallery-img"
            style="max-width: 65px; border-radius: 8px;" />
        </a>
        <img src="diagrams/1.2-dataset-examples/placeholder.png"/>
      </div>
      <figcaption class="figcaption">
        <a href="#1.2.0-dataset-examples" class="figure-number">2</a>:
        Crops taken from Imagenet<d-cite bibtex-key="imagenet5206848"></d-cite> where <a href="https://microscope.openai.com/models/inceptionv1/mixed3a_0/136"><code>mixed3a</code> 136</a> activated maximally,
        argmaxed over spatial locations.
      </figcaption>
    </figure>

    <p>A wide range of real-world situations can cause high-low frequency detectors to fire. Oftentimes it's a highly-textured, in-focus foreground object against a blurry background &mdash; for example, the foreground might be the microphone's latticework, the hummingbird's tiny head feathers, or the small rubber dots on the Lenovo ThinkPad <a href="https://en.wikipedia.org/wiki/Pointing_stick">pointing stick</a> &mdash; but not always: we also observe that it fires for the MP3 player's brushed metal finish against its shiny screen, or the text of a watermark.</p>

    <p>In all cases, we see one area with high frequency and another area with low frequency. Although they often fire at an object boundary,

      they can also fire in cases where there is a frequency change without an object boundary.

       High-low frequency detectors are therefore not the same as <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">boundary detectors</a>.</p>

    <a href="#tuning-curves" class="header-self-link">
      <h3 id="tuning-curves">Synthetic Tuning Curves</h3>
    </a>

    <p>
      <b>Tuning curves</b> show us how a neuron's response changes with respect to a parameter.

      They are a standard method in neuroscience<d-cite bibtex-key="hubel1962receptive"></d-cite>, and we've found them very helpful for studying artificial neural networks as well. For example, we used them to demonstrate <a href="https://distill.pub/2020/circuits/curve-detectors/#radial-tuning-curve">how the response of curve detectors changes</a> with respect to orientation.

      Similarly, we can use tuning curves to show how high-low frequency detectors respond.
    </p>

    <p>
      To construct such a curve, we'll need a set of <i>synthetic stimuli</i> which cause high-low frequency detectors to fire.

      We generate images with a high-frequency pattern on one side and a low-frequency pattern on the other. Since we're interested in orientation, we'll rotate this pattern to create a 1D family of stimuli:
    </p>

    <figure id="1.4-tuning-curves-space-axis-1">
      <img src="diagrams/1.4-tuning-curves/orientation.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        The first axis of variation of our synthetic stimuli is <em>orientation</em>.
      </figcaption>
    </figure>

    <p>
      But what frequency should we use for each side? How steep does the difference in frequency need to be?
      To explore this, we'll add a second dimension varying the ratio between the two frequencies:
    </p>

    <figure id="1.4-tuning-curves-space-axis-2">
      <img src="diagrams/1.4-tuning-curves/ratio.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        The second axis of variation of our synthetic stimuli is the <em>frequency ratio</em>.
        <!--  on the two sides of the high-low frequency edge. -->
      </figcaption>
    </figure>

    <p>
      (Adding a second dimension will also help us see whether the results for the first dimension are robust.)
    </p>

    <p>
      Now that we have these two dimensions, we sample the synthetic stimuli and plot each neuron's responses to them:</p>

    <d-figure id="figure-3"><figure style="display: grid;"></figure></d-figure>
    <p>
      Each high-low frequency detector exhibits a clear preference for a limited range of orientations.

      As we <a href="https://distill.pub/2020/circuits/curve-detectors/#synthetic-curves">previously found</a> with curve detectors, high-low frequency detectors are rotationally <a href="https://distill.pub/2020/circuits/equivariance/">equivariant</a>: each one selects for a given orientation, and together they span the full 360º space.
    </p>

    <!-- Alternate title: Upstream Circuits -->
    <!-- Alternate alternate title: Feature Implementation -->
    <a class="header-self-link" href="#implementation">
      <h2 id="implementation">Implementation</h2>
    </a>
    <p>How are high-low frequency detectors built up from lower-level neurons?

      One could imagine many different circuits which could implement this behavior. To give just one example, it seems like there are at least two different ways that the oriented nature of these units could form.
    </p>

    <ul>
      <li>
        <b>Equivariant→Equivariant Hypothesis.</b> The first possibility is that the previous layer already has precursor features which detect oriented transitions from high frequency to low frequency. The extreme version of this hypothesis would be that the high-low frequency detector is just an identity passthrough of some lower layer neuron. A more moderate version would be something like what we see with curve detectors, where <a href="https://distill.pub/2020/circuits/early-vision/#group_conv2d2_tiny_curves">early curve detectors</a> become refined into the larger and more sophisticated <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_curves">late curve detectors</a>. Another example would be how edge detection is built up from simple <a href="https://distill.pub/2020/circuits/early-vision/#group_conv2d0_gabor_filters">Gabor filters</a> which were already oriented.

        We call this <a href="https://distill.pub/2020/circuits/equivariance/#equivariant-to-equivariant">Equivariant→Equivariant</a> because the equivariance over orientation was already there in the previous layer.
      </li>
      <li>
        <b>Invariant→Equivariant Hypothesis.</b> Alternatively, previous layers might not have anything like high-low frequency detectors. Instead, the orientation might come from spatial arrangements in the neuron's weights that govern where it is excited by low-frequency and high-frequency features.
      </li>
    </ul>

    <p>To resolve this question &mdash; and more generally, to understand how these detectors are implemented &mdash; we can look at the weights.</p>

    <p>Let's look at a single detector. Glancing at the weights from <a href="https://distill.pub/2020/circuits/early-vision/#conv2d2"><code>conv2d2</code></a> to <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_110.html"><code>mixed3a</code> 110</a>, most of them can be roughly divided into two categories: those that activate on the left and inhibit on the right, and those that do the opposite.
    </p>

    <d-figure id="figure-4">
      <figure >
        <%= require('../static/diagrams/figure-4.svg') %>
        <figcaption id="figure-4-caption">
            <a href="#figure-4" class="figure-number">4</a>:
            Six neurons from conv2d2 contributing weights to mixed3a 110.

            <!--Neurons in the top row <span class="legend-label support-rb">activate</span> on the left and <span class="legend-label inhibit-rb">inhibit</span> on the right; neurons in the bottom row <span class="legend-label inhibit-rb">inhibit</span> on the left and <span class="legend-label support-rb">activate</span> on the right.-->
        </figcaption>
      </figure>
    </d-figure>

    <p>
      The same also holds for each of the other high-low frequency detectors &mdash; but, of course, with different spatial patterns<d-footnote>As an aside: The 1-2-1 pattern on each column of weights is curiously reminiscent of the structure of the <a href="https://en.wikipedia.org/wiki/Sobel_operator">Sobel filter</a>.</d-footnote> on the weights, implementing the different orientations.
    </p>

    <p>
      Surprisingly, across all high-low frequency detectors, the two clusters of neurons that we get for each are actually the <i>same</i> two clusters! One cluster appears to detect textures with a generally high frequency, and one cluster appears to detect textures with a generally low frequency.
    </p>
    <d-figure id="figure-5">
      <figure style=''>
        <!-- <%= require("../static/images/Underlying-Weights.svg") %> -->
        <!-- <img src="images/Underlying-Weights.svg" /> -->
        <!-- <%= require('../static/images/Underlying-Weights.svg') %> -->
        <img src="diagrams/HF-LF-clusters-amd-weight-structure.png" style="width: 100%; max-width: 549px;"/>
        <figcaption>
          <p>
            <a href="#figure-5" class="figure-number">5</a>:
            The strongest weights on any high-low frequency detector (here shown: <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_110.html"><code>mixed3a</code> 110</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_136.html"><code>mixed3a</code> 136</a>, and <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_112.html"><code>mixed3a</code> 112</a>) can be divided into roughly two clusters. Each cluster contributes its weights in similar ways.
          </p>
          <p>
            Top row: underlying neurons <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_119.html"><code>conv2d2</code> 119</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_102.html"><code>conv2d2</code> 102</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_123.html"><code>conv2d2</code> 123</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_90.html"><code>conv2d2</code> 90</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_89.html"><code>conv2d2</code> 89</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_163.html"><code>conv2d2</code> 163</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_98.html"><code>conv2d2</code> 98</a>, and <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_188.html"><code>conv2d2</code> 188</a>.
          </p>
        </figcaption>
      </figure>
    </d-figure>

    <p>
      This is exactly what we would expect to see if the Invariant→Equivariant hypothesis is true: each high-low frequency detector composes the same two components in different spatial arrangements, which then in turn govern the detector's orientation.
    </p>
    <p>
      These two different clusters are really striking.

      In the next section, we'll investigate them in more detail.
    </p>

    <h3 id="hf-lf-factors">High and Low Frequency Factors</h3>

    <p>It would be nice if we could confirm that these two clusters of neurons are real. It would also be nice if we could create a simpler way to represent them for circuit analysis later.</p>

    <p>Factorizing the connections<d-footnote>Between two adjacent layers, "connections" reduces to the weights
        between the two layers. Sometimes we are interested in observing connectivity between layers that may not be
        directly adjacent. Because our model, a deep convnet, is non-linear, we will need to approximate the
        connections. A simple approach that we take is to linearize the model by removing the non-linearities. While
        this is not a great approximation of the model's behavior, it does give a reasonable intuition for
        counterfactual influence: had the neurons in the intermediate layer fired, how it would have affected neurons in
        the downstream layers. We treat positive and negative influences separately.</d-footnote> between lower layers and the high-low frequency detectors is one way that we can check whether these two clusters are meaningful, and investigate their significance. Performing a one-sided non-negative matrix factorization (NMF)<d-footnote>We require that the channel factor be positive, but allow the spatial factor to have both positive and negative values.</d-footnote> separates the connections into two factors.</p>
    <p>Each factor corresponds to a vector over neurons. Feature visualization can also be used to visualize these linear combinations of neurons. Strikingly, one clearly displays a generic high-frequency image, whereas the other does the same with a low-frequency image.<d-footnote>In InceptionV1 in particular, it's possible that we recover these two factors so crisply in part due to the <i><a href="https://microscope.openai.com/models/inceptionv1/mixed3a_3x3_bottleneck_0?models.op.feature_vis.type=neuron&models.op.technique=feature_vis">3x3 bottleneck</a></i> between conv2d2 and mixed3a. Because of this, we're not here looking at direct weights between conv2d2 and mixed3a, but rather the "expanded weights," which are a product of a 1x1 convolution (which reduces down to a small number of neurons) combined with a 3x3 convolution. This structure is very similar to the factorization we apply. However, as we see later in <a href="#universality">Universality</a>, we recover similar factors for other models where this bottleneck doesn’t exist. NMF makes it easy to see this abstract circuit across many models which may not have an architecture that more explicitly reifies it.</d-footnote> We'll call these the <em>HF-factor</em> and the <em>LF-factor</em>:</p>

    <d-figure id="figure-6">
      <%= require('./diagrams/upstream-neurons.ejs')() %>
    </d-figure>

    <p>The feature visualizations are suggestive, but how can we be sure that these factors really correspond to high and low frequency in general, rather than specific high or low frequency patterns? One thing we can do is to create synthetic stimuli again, but now plotting the responses of those two NMF factors.
    </p>
    <p>

    Since our factors don't correspond to an edge, our synthetic stimuli will only have one frequency region for each stimulus. To add a second dimension and again demonstrate robustness, we also vary the rotation of that region. (The frequency texture is not exactly rotationally invariant because we construct the stimulus out of orthogonal cosine waves.)
    </p>

    <d-figure id="figure-7"><figure></figure></d-figure>

    <p>Unlike last time, these activations now mostly ignore the image's orientation, but are sensitive to its frequency. We can average these results over all orientations in order to produce a simple tuning curve of how each factor responds to frequency. As predicted, the HF-factor responds to high frequency and the LF-factor responds to low frequency.</p>

    <d-figure id="figure-8">
      <figure>
        <%= require('../static/diagrams/hf-lf-responses.svg') %>
        <!-- <img src="diagrams/hf-lf-responses.svg" style="margin-bottom: 20px;"/> -->
      <figcaption>
          <p>
            <a href="#figure-8" class="figure-number">8</a>:
            Tuning curve for HF-factor and LF-factor from <code>conv2d2</code> against images with synthetic frequency, averaged across orientation. Wavelength as a proportion of the full input image ranges from 1:1 to 1:10.
          </p>
      </figcaption>
      </figure>
    </d-figure>


    <p>
      Now that we've confirmed what these factors are, let's look at how they're combined into high-low frequency detectors.
    </p>

    <h3>Construction of High-Low Frequency Detectors</h3>

    <p>
      NMF factors the weights into both a channel factor and a spatial factor. So far, we've looked at the two parts of the channel factor. The spatial factor shows the spatial weighting that combines the HF and LF factors into high-low frequency detectors.

    </p>
    <p>

      Unsurprisingly, these weights basically reproduce the same pattern that we'd previously been seeing in <a href="#figure-5">Figure 5</a> from its two different clusters of neurons: where the HF-factor inhibits, the LF-factor activates &mdash; and vice versa.
      <d-footnote>
        As an aside, the HF-factor here for InceptionV1 (as well as some of its NMF components, like <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_123.html"><code>conv2d2</code> 123</a>) also appears to be lightly activated by bright greens and magentas. This might be responsible for the feature visualizations of these high-low frequency detectors showing only greens and magentas on the high-frequency side.
      </d-footnote>
    </p>

    <%= require('./diagrams/upstream-nmf.ejs')() %>

    <p>High-low frequency detectors are therefore built up by circuits that arrange high frequency detection on one side and low frequency detection on the other.</p>

    <p>
      There are some exceptions that aren't fully captured by the NMF factorization perspective. For example, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_181.html"><code>conv2d2</code> 181</a> is a texture contrast detector that appears to already have spatial structure.

      This is the kind of feature that we would expect to be involved through an Equivariant→Equivariant circuit.

      If that were the case, however, we would expect its weights to the high-low frequency detector <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_70.html"><code>mixed3a</code> 70</a> to be a solid positive stripe down the middle.

      What we instead observe is that it contributes as a component of high frequency detection, though perhaps with a slight positive overall bias.


       Although <code>conv2d2</code> 181 has a spatial structure, perhaps it responds more strongly to high frequency patterns.

     </p>

    <d-figure>
      <figure>
        <img src="diagrams/2d2-181-3a-70-weight.png" style="width: 100%; max-width: 320px; margin: auto; margin-bottom: 20px; display: block; image-rendering: pixelated;"/>
      <figcaption>
          The weights from <code>conv2d2</code> 181 to <code>mixed3a</code> 70 are consistent with <code>conv2d2</code> 181 contributing via the HF-factor, not via the existing spatial structure of its texture contrast detection.
      </figcaption>
      </figure>
    </d-figure>

    <p>Now that we understand how they are constructed, how are high-low frequency detectors used by higher-level features?</p>

    <!-- Alternate title: Downstream Circuits -->
    <!-- Alternate alternate title: Feature Usage -->
    <a class="header-self-link" href="#usage">
      <h2 id="usage">Usage</h2>
    </a>

    <!-- <h3>In mixed3b</h3> -->

    <!-- <d-figure>
      <figure>
        <img src="diagrams/downstream-interpretation.svg" style="margin-bottom: 20px; display: block;"/>
      <figcaption>
          <p>(TODO fix this diagram still!!) Some of <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_314.html"><code>mixed3b</code> 314</a>'s high-low frequency detector weights in more explicit detail.</p>
          <p>One of the functions of high-low frequency detectors appears to be to indicate discontinuities between different objects or regions, as well as an object's continuity with itself.
          </p>
      </figcaption>
      </figure>
    </d-figure> -->

    <p>
      <a href="https://distill.pub/2020/circuits/early-vision/#mixed3b"><code>mixed3b</code></a> is the next layer immediately after the high-low frequency detectors. Here, high-low frequency detectors contribute to a variety of features. Their most important role seems to be supporting <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">boundary detectors</a>, but they also contribute to <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_bumps">bumps</a> and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_divots">divots</a>, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_bar_line_like">line-like</a> and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_curves_misc.">curve-like</a> shapes,
      and at least one each of <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_281.html">center-surrounds</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_372.html">patterns</a>, and <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_276.html">textures</a>.
    </p>

    <d-figure style="grid-column-end: page-end" id="figure-10">
      <figure>
        <!-- <%= require('../static/diagrams/downstream-circuits.svg') %> -->
        <img src="diagrams/usage-1.png" />
        <!-- <img src="diagrams/downstream-circuits.svg" style="margin-bottom: 20px"/> -->
      <figcaption style="grid-column-end: text-end">
        <p>
            <a href="#figure-10" class="figure-number">10</a>:
            Examples of neurons that high-low frequency detectors contribute to: <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_365.html"><code>mixed3b</code> 365</a> (an hourglass shape detector), <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_276.html"><code>mixed3b</code> 276</a> (a center-surround texture detector), and <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_314.html"><code>mixed3b</code> 314</a> (a double boundary detector).
        </p>
      </figcaption>
      </figure>
    </d-figure>

    <p>
      Oftentimes, downstream features appear to ignore the "polarity" of a high-low frequency detector, responding roughly the same way regardless of which side is high frequency. For example, the vertical boundary detector <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_345.html"><code>mixed3b</code> 345</a> (see above) is strongly excited by high-low frequency detectors that detect frequency change across a vertical line in either direction.
    </p>
      <p>
        Whereas activation from a high-low frequency detector can help detect boundaries between different objects, inhibition from a high-low frequency detector can also add structure to an object detector by detecting regions that must be contiguous along some direction &mdash; essentially, indicating the absence of a boundary.
    </p>

    <d-figure id="figure-11">
      <figure>
        <img src="diagrams/usage-2.png" style="width: 100%; max-width: 394px; margin-bottom: 20px;"/>
      <figcaption>
        <p>
          <a href="#figure-11" class="figure-number">11</a>:
          Some of <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_314.html"><code>mixed3b</code> 314</a>'s weights, extracted for emphasis. Orientation doesn't matter so much for how these weights are used by <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_314.html"><code>mixed3b</code> 314</a>, but their 180º-invariant orientation does!</p>
        <p>You may notice that strong <span class="legend-label support-rb">excitation</span> (left) is correlated with the presence of a <b>boundary</b> at a particular angle, whereas strong <span class="legend-label inhibit-rb">inhibition</span> (right) is correlated with <b>object continuity</b> where a boundary might otherwise have been.</p>
      </figcaption>
      </figure>
    </d-figure>

    <!-- <%= require('./diagrams/downstream-neurons.ejs')() %> -->

    <p>As we've mentioned, by far the primary downstream contribution of high-low frequency detectors is to <i>boundary detectors</i>. Of the top 20 neurons in <code>mixed3b</code> with the highest L2-norm of weights across all high-low frequency detectors, eight of those 20 neurons participate in boundary detection of some sort: <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_double_boundary">double boundary detectors</a>, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary_misc">miscellaneous boundary detectors</a>, and especially <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">object boundary detectors</a>. </p>

    <h4>Role in object boundary detection</h4>

    <p><a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">Object boundary detectors</a></i> are neurons which detect boundaries between objects, whether that means the boundary between one object and another or the transition from foreground to background. Object detectors are not the same as edge detectors or curve detectors: although they are sensitive to edges (indeed, some of their strongest weights are contributed by lower-level edge detectors!), object boundary detectors are also sensitive to other indicators such as color contrast and high-low frequency detection.</p>


    <figure id="figure-12">
      <img src="diagrams/usage-boundary.png" />
      <figcaption style="width: 80%; position: absolute; left: 20%; top: 75%;">
        <a href="#figure-12" class="figure-number">12</a>: <code>mixed3b</code> 345 is a boundary detector activated by high-low frequency detectors, edges, color contrasts, and end-of-line
        detectors. It is specifically sensitive to vertically-oriented high-low frequency detectors, regardless of their
        orientation, and along a vertical line of positive weights.
      </figcaption>
    </figure>

    <p> High-low frequency detectors contribute to these object boundary detectors by providing one piece of evidence that an object has ended and something else has begun. Some examples of object boundary detectors are shown below, along with their weights to a selection of high-low frequency detectors, grouped by orientation (ignoring polarity).</p>

    <p>In particular, note how similar the weights are within each grouping! This shows us again that the later layers ignore the high-low frequency detectors' polarity. Furthermore, the arrangement of excitatory and inhibitory weights contributes to each boundary detector's overall shape, following the principles outlined above.</p>


    <d-figure id="figure-13">
      <figure>
        <style>
          /* TODO: Optimize smaller breakpoints by hand */
        </style>
        <img src="diagrams/usage-boundaries.png" style="margin-bottom: 20px;"/>
        <figcaption>
          <a href="#figure-13" class="figure-number">13</a>: Four examples of object boundary detectors that high-low frequency detectors contribute to: <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_345.html"><code>mixed3b</code> 345</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_376.html"><code>mixed3b</code> 376</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_368.html"><code>mixed3b</code> 368</a>, and <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_151.html"><code>mixed3b</code> 151</a>.
        </figcaption>
      </figure>
    </d-figure>

    <!-- <p class="todo">Show cosine similarity of forward weights as compared to incoming weights; expect -1 incoming; 1 outgoing. Can put this in a table?</p> -->

    <!-- <h3>In mixed4a</h3> -->

    <p>
      Beyond <code>mixed3b</code>, high-low frequency detectors ultimately play a role in detecting more sophisticated object shapes in <code>mixed4a</code> and beyond, by continuing to contribute to the detection of boundaries and contiguity.
    </p>

    <!-- <figure id="3.2-downstream-nmf">
      <img src="diagrams/3.2-downstream-nmf/placeholder.png" />
      <figcaption class="figcaption">
        <p>8 neurons from <code>mixed4a</code> (top row) with the highest L2-norm of weights on the given 9 boundary detectors (left column) from <code>mixed3a</code>, along with weights: <span class="legend-label support">activation</span> and <span
        class="legend-label inhibit">inhibition</span>.</p>
      </figcaption>
    </figure> -->

    <p>
    So far, the scope of our investigation has been limited to InceptionV1.

    How common are high-low frequency detectors in convolutional neural networks generally?</p>

    <!--Universality-->
    <h2 id="universality">Universality</h2>

    <h3>High-Low Frequency Detectors in Other Networks</h3>

    <p>It's always good to ask if what we see is the rule or an interesting exception<d-cite bibtex-key="li2015convergent"></d-cite>&mdash;and high-low frequency detectors seem to be the rule.
    High-low frequency detectors similar to ones in InceptionV1 can be found in a variety of architectures.
    </p>

    <%= require('./diagrams/similar-directions.ejs')() %>

    <p>Notice that these detectors are found at very similar depths within the different networks, between 29% and 33% network depth! While the particular orientations each network's high-low frequency detectors respond to may vary slightly, each network has its own family of detectors that together cover the full 360º and comprise a rotationally <a href="https://distill.pub/2020/circuits/equivariance/">equivariant</a> family.
      <d-footnote>Architecture aside – what about networks trained on substantially different <i>datasets</i>? In the extreme case, one could imagine a synthetic dataset where high-low frequency detectors don't arise. For most practical datasets, however, we expect to find them. For example, we even find some candidate high-low frequency detectors in AlexNet (Places): <a href="https://microscope.openai.com/models/alexnet_caffe_places365/conv2_Conv2D_0/37">down-up</a>, <a href="https://microscope.openai.com/models/alexnet_caffe_places365/conv2_Conv2D_0/91">left-right</a>, and <a href="https://microscope.openai.com/models/alexnet_caffe_places365/conv2_Conv2D_0/104">up-down</a>.</d-footnote>
    </p>

    <p>Even though these families are from three completely different networks, we also discover that their high-low frequency detectors are built up from high and low frequency components.</p>

    <h3>HF-factor and LF-factor in Other Networks</h3>

    <p>
      As we did with InceptionV1, we can again perform NMF on the weights of the high-low frequency detectors in each network in order to extract the strongest two factors.
    </p>

    <%= require('./diagrams/universality.ejs')() %>


    <p>The feature visualizations of the two factors reveal one clear HF-factor and one clear LF-factor, just like what we found in InceptionV1. Furthermore, the weights on the two factors are again very close to symmetric.</p>
    <p>Our earlier conclusions therefore also hold across these different networks: high-low frequency detectors are built up from the specific spatial arrangement of a high frequency component and a low frequency component.</p>

    <h2>Conclusion</h2>

    <p>Although high-low frequency detectors represent a feature that we didn't necessarily expect to find in a neural network, we find that we can still explore and understand them using the interpretability tools we've built up for exploring circuits: NMF, feature visualization, synthetic stimuli, and more.

    <p>We've also learned that high-low frequency detectors are built up from comprehensible lower-level parts, and we've shown how they contribute to later, higher-level features.

      Finally, we've seen that high-low frequency detectors are common across multiple network architectures.
    </p>

    <p>Given the universality observations, we might wonder whether the existence of high-low frequency detectors isn't so unnatural after all. We even find <a href="https://microscope.openai.com/models/alexnet_caffe_places365/conv2_Conv2D_0/91">approximate</a> high-low frequency detectors in AlexNet Places, with its substantially different training data. Beyond neural networks, the aesthetic quality imparted by the blurriness of an out-of-focus region of an image is already known as to photographers <a href="https://en.wikipedia.org/wiki/Bokeh"><i>bokeh</i></a>. And in VR, visual blur can either provide an effective depth-of-field cue or, conversely, can induce nausea in the user when implemented in a dissonant way. Perhaps frequency detection might well be commonplace in both natural and artificial vision systems as yet another type of informational cue.</p>

    <p>Nevertheless, whether their existence is natural or not, we find that high-low frequency detectors are possible to characterize and understand.</p>

  </d-article>

  <d-appendix>
    <h3>Author Contributions</h3>

    <p>As with many scientific collaborations, the contributions to the high-low frequency detectors paper are difficult to separate because it was a collaborative effort that we wrote together.</p>

    <p><b>Conceptual Contributions.</b> Christopher Olah originally noted the high-low frequency directors as a research direction. </p>

    <p><b>Experiments.</b> Ludwig Schubert wrote the code for generating and measuring synthetic tuning curves for the high-low frequency detectors, for performing NMF on the high-low frequency detectors to extract the HF-factor and the LF-factor, and for performing NMF on the high-low frequency detectors from other networks. Chelsea Voss wrote the code for generating and measuring synthetic stimuli for the HF-factor and LF-factor, with help from Chris for extracting and using the NMF components.</p>

    <p><b>Figures.</b> Ludwig designed the visualization of the high-low frequency detector synthetic tuning curves, the visualization of the HF-factor and LF-factor NMF vectors, the visualization of the HF-factor and LF-factor NMF weights from conv2d1 and conv2d2, and the figures demonstrating high-low frequency detectors from other networks shown in the Universality section. Chelsea designed the visualization of the HF-factor and LF-factor synthetic stimuli results and the figures articulating the downstream use of high-low frequency detectors, and edited some of the final figures. Two figures were borrowed from Zoom In and Early Vision.

    <p><b>Writing.</b> Chelsea and Ludwig wrote the paper, with feedback from Chris.</p>

    <!--Potential Limitations-->
    <!-- <h3>Limitations</h3>
    <h4>How bad is the linearization?</h4>
    <p class="todo">Maybe we can measure and put up some <em>summary statistics</em> on how bad our linear approximation
      really is.
      Maybe in terms of % variance of resulting activation? -> colah if there's sth more principled.</p>

    <figure id="5.1-linearization-badness">
      <img src="diagrams/5.1-linearization-badness/placeholder.jpg" />
      <figcaption class="figcaption">
        Maybe show % of variance explained over distance between layers or sth; if so likely move to appendix.
      </figcaption>
    </figure> -->


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
