<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1><span style="display: inline; color: gray; margin-right: 0.25em">2.4</span>High/Low Frequency Detectors</h1>
    <p>A family of early-vision filters reacting to contrasts between spatial gratings of different frequency</p>
    <figure id="hero" class="l-body todo">I'm just a figure who's a hero for fun.</figure>
  </d-title>

  <d-article>

    <h2 style="display: none;">Introduction</h2>
    <p>
      Why should you study awkwardly-named "high/low frequency detectors" when we've just walked you through curve
      detectors in detail? There's a strategic argument from the author's side: everybodyÂ® would guess that vision
      networks contain curve detectors. It's easy to imagine how they'are built up from earlier edge detectors, and
      they're common enough that neuroscientists routinely find them in animal visual corti. "High/low frequency
      detectors", on the other hand, form a family of features we did not expect to find. Still, they have many of the
      hallmarks of important features: their existence feels natural in retrospect, they're an important part of later,
      higher-level features, and they're common across network architectures.
    </p>


    <!--Empirical observation-->
    <h2>Empirical Observation</h2>

    <h4>Feature Visualization</h4>
    <p>We originally found high/low frequency detectors by studying models' neurons' <a
        href="//distill.pub/2017/feature-visualization">feature visualizations</a>: synthetic inputs
      optimized to elicit maximal activation of a single, specific neuron.</p>
    <style>
      .gallery {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(28px, 56px));
        grid-gap: 0.5rem;
        justify-content: start;
      }

      ul.gallery {
        padding-left: 0;
      }

      .gallery img {
        max-width: 100%;
        width: unset;
        object-fit: none;
        object-position: center;
        border-radius: 0.25rem;
      }

      @media screen and (min-width: 768px) {
        .gallery {
          grid-template-columns: repeat(7, minmax(28px, 112px));
          grid-gap: 1rem;
          justify-content: center;
        }
      }
    </style>
    <figure class="l-screen base-grid" id="1.1-feature-vis">
      <ul class="gallery l-page">
        <% for (const unit of require("../static/diagrams/1.1-feature-vis/units.json").units) {%>
        <img src="<%= `diagrams/1.1-feature-vis/neuron-${unit}.png` %>" title="<%= "Unit " + unit %>" />
        <% } %>
      </ul>
      <figcaption class="figcaption l-body">
        <a href="#1.1-feature-vis" class="figure-number">1</a>:
        A variety of detectors from InceptionV1's <code>mixed3a</code> layer.
      </figcaption>
    </figure>

    <h4>Dataset Examples</h4>
    <p>
      To ensure we're not misreading those feature visualizations it's good practice to check what such filters
      maximally activate on over a natural data distribution, say the training set.
    </p>

    <figure id="1.2-dataset-examples">
      <img src="diagrams/1.2-dataset-examples/placeholder.png" />
      <figcaption class="figcaption">
        <a href="#1.2-dataset-examples" class="figure-number">2</a>:
        Crops taken from training dataset, Imagenet, on which a High-Low Frequency detector activated maximally;
        argmaxed over spatial locations.
      </figcaption>
    </figure>

    <p>
      The dataset examples show that a wide range of real-world objects can cause these detectors to fire: deep 3D
      textures, such as on the microphone against a blurred background, but also surface textures such as the MP3
      player's metal finish against it's shiny screen.
    </p>

    <h4>Tuning Curves</h4>
    <p>
      In the curve detector chapter we saw an example of a tuning curve: it plots filter response over rotation angle,
      where the input is a natural image that we know has curvature in it. We can repeat this experiment for high/low
      frequency detectors: since their response is also specific to an angle, we expect to see a similar pattern of
      angle-specific activation. To measure this, we create artificial stimuli: images that contain a high-low frequency
      edge at a known orientation:
    </p>

    <figure id="1.4-tuning-curves-space-axis-1">
      <img src="diagrams/1.4-tuning-curves/orientation.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        The first axis of variation of our artificial stimuli is <em>orientation</em>.
      </figcaption>
    </figure>

    <p>
      The curve detector tuning curve also varied a second parameter, arc length. For
      high-low frequency detectors, we will instead vary the ratio between the two frequencies:
    </p>

    <figure id="1.4-tuning-curves-space-axis-2">
      <img src="diagrams/1.4-tuning-curves/ratio.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        The second axis of variation of our artificial stimuli is the <em>ratio between frequencies</em>.
        <!--  on the two sides of the high-low frequency edge. -->
      </figcaption>
    </figure>

    <p>These two axes define a space from which we can sample image stimuli: every 2D point corresponds to a combination
      of
      the two varied attributes. In the tuning curve diagram below, each unit is associated with a 60 by 360 plot of
      activation strength. In those plots, each pixel corresponds to one of the artificial stimuli.</p>

    <!-- <figure id="1.4-tuning-curves-samples">
      <img src="diagrams/1.4-tuning-curves/samples.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        TODO:
      </figcaption>
    </figure> -->

    <!-- <p>
      But high/low frequency detectors seem to have more knobs to turn than just angle: the frequencies of the two
      gratings that make up the frequency edge should also affect the filters' responses. Let's measure activations over
      a space that varies both of those, as well as orientation: Angle by frequency 1 by frequency 2. We will then have
      to visualize this volume in various ways to show it on this 2D screen.
    </p> -->

    <p class="todo done">Legend @shan; smarter ideas? No responsiveness? etc.</p>

    <d-figure id="1.4-tuning-curves">
      <figure></figure>
    </d-figure>

    <p>
      By argmaxing over the two frequencies we recover the angle-response curve from the previous section:
    </p>


    <!--Upstream Circuits-->
    <h2>Upstream Circuits</h2>
    <p>How are High-Low frequency detectors constructed from lower-level features?</p>

    <p>A natural guess would be to assume these filters combine low frequency detectors and high frequency detectors.
      Let us factorize the connections<d-footnote>Between two adjacent layers, "connections" reduces to the weights
        between the two layers. Sometimes we are interested in observing connectivity between layers that may not be
        directly adjacent. Because our model, a deep convnet, is non-linear, we will neeed to approximate the
        connections. A simple approach that we take is to linearize the model by removing the non-linearities. While
        this is not a great approximation of the model's behavior, it does give a reasonable intuition for
        counterfactual influence: had the neurons in the intermediate layer fired, how would it have affected neurons in
        the downstream layers. We treat positive and negative influences separately.</d-footnote> between lower level
      features and the High-Low frequency detectors we already found:</p>

    <p>NMFing the weights into 2 factors recovers low/high frequency neuron clusters.</p>

    <%= require('./diagrams/upstream-nmf.ejs')() %>

    <p class="todo"> talk more explicitly about above diagram</p>

    <h4>Underlying Neurons title tbd</h4>
    
    <p>Let's inspect which lower-level neurons primarily contribute to those NMF factors:</p>

    <%= require('./diagrams/upstream-neurons.ejs')() %>

    <h4>Circuit diagram</h4>

    <p>To understand how these detectors are implemented in terms of even earlier features, we can look at which
      features feed into a given high/low frequency detectoer, and even at the spatial structure of the weights
      connecting those features to the detector in question.</p>

    <figure id="2.1-upstream-circuit">
      <img src="diagrams/3.1-downstream-circuit/placeholder.png" />
      <figcaption class="figcaption">
        TODO: replace with Nick's interactive version on integration?
      </figcaption>
    </figure>

    <h4>Joint NMF of weights</h4>



    <!--Downstream Circuits-->
    <h2>Downstream Circuits</h2>
    <p>How are High-Low frequency detectors used in the construction of higher-level features?</p>

    <figure id="3.1-downstream-circuit">
      <img src="diagrams/3.1-downstream-circuit/placeholder.png" />
      <figcaption class="figcaption">
        TODO: replace with Nick's interactive version on integration?
      </figcaption>
    </figure>

    <p>Object boundary detectors, and how they're dissimilar from edge detectors/their invariances</p>
    <p>Show how opposing high/low frequency detectors are used to make boundary detectors invariant to orientiation</p>
    <p>Show cosine similarity of forward weights as compared to incoming weights; expect -1 incoming; 1 outgoing.</p>

    <figure id="3.2-downstream-nmf">
      <img src="diagrams/3.2-downstream-nmf/placeholder.png" />
      <figcaption class="figcaption">
        TODO: Maybe have better ideas after doing 2.2 NMF diagram
      </figcaption>
    </figure>

    <p class="todo">Maybe use cosine similarity overlay to show how object boundaries get detected by such detectors.
    </p>


    <!--Universality-->
    <h2>Universality</h2>
    <p>How common are High-Low frequency detectors in convolutional neural networks?</p>
    <p>Always good to ask if what you see is the rule or an interesting exception. high/low frequency detectors seem to
      be the rule.</p>

    <h4>High/Low frequency detectors in other models</h4>

    <%= require('./diagrams/similar-directions.ejs')() %>

    <h4>Tuning curves in other models</h4>
    <p>Show tuning curves!</p>

    <figure id="4.2-similar-detectors">
      <img src="diagrams/4.2-similar-detectors/placeholder-1.jpg" />
      <figcaption class="figcaption">
        Top k filters responding to stimuli from earlier section
      </figcaption>
    </figure>

    <h4>Re-occurence at deeper layers at different scale</h4>
    <p>Show re-occurence at higher layers at different scales?</p>

    <figure id="4.3-higher-layers">
      <img src="diagrams/4.3-higher-layers/placeholder.png" />
      <figcaption class="figcaption">
        Top k filters responding to stimuli from earlier section. Compare their response to earlier layer hilo
        detectors; ideally show frequency gap / non-huge-overlap in response 2D tuning curve.
      </figcaption>
    </figure>


    <h2>Conclusion</h2>
    <p>?</p>

  </d-article>



  <d-appendix>
    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p>

    <!--Potential Limitations-->
    <h3>Limitations</h3>
    <h4>How bad is the linearization?</h4>
    <p class="todo">Maybe we can measure and put up some <em>summary statistics</em> on how bad our linear approximation
      really is.
      Maybe in terms of % variance of resulting activation? -> colah if there's sth more principled.</p>

    <figure id="5.1-linearization-badness">
      <img src="diagrams/5.1-linearization-badness/placeholder.jpg" />
      <figcaption class="figcaption">
        Maybe show % of variance explained over distance between layers or sth; if so likely move to appendix.
      </figcaption>
    </figure>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>