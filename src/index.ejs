<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>
  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <style>
  .header-self-link {
    border-bottom: none;
  }
  .header-self-link:hover {
    border-bottom: none;
  }
  </style>

  <d-title>
    <h1>High-Low Frequency Detectors</h1>
    <p>A family of early-vision neurons reacting to adjacent, sinusoidally-modulated gratings of different spatial frequencies.</p>
    <figure id="hero" class="l-body">
      <img src="images/high-low-hero.png"/>
    </figure>
  </d-title>

  <d-article>

    <h2 style="display: none;">Introduction</h2>

    <p>Some of the neurons in <a href="https://distill.pub/2020/circuits/early-vision/">early vision</a> are features that we aren't particularly surprised to find. <a href="https://distill.pub/2020/circuits/curve-detectors/">Curve detectors</a>, for example, had already been discovered in the animal visual cortex<d-cite bibtex-key="tang2018complex,pasupathy2001shape,jiang2019discrete"></d-cite>. It's easy to imagine how curve detectors are built up from earlier edge detectors, and it's easy to guess why curve detection might be useful to the rest of the neural network.</p>

    <p>High-low frequency detectors, on the other hand, seem more surprising. They are not a feature that we would have expected <i>a priori</i> to find. Yet, when systematically characterizing<d-cite bibtex-key="olah2020an"></d-cite> the first five layers of InceptionV1<d-cite bibtex-key="szegedy2015going"></d-cite>, we found a full <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_high_low_frequency">15 neurons</a> of <code>mixed3a</code> that appear to detect a high frequency pattern on one side, and a low frequency pattern on the other.</p>

    <p>
      These detectors are not quite as intuitive as curve detectors.

      However, <a href="https://distill.pub/2020/circuits/zoom-in/">we've claimed</a> that features and circuits can be rigorously studied and understood<d-cite bibtex-key="olah2020zoom"></d-cite> – and that should include the unintuitive ones! We could have discovered that only intuitive features are interpretable, but instead, here we find the opposite: using the same tools we've built up for exploring circuits, we find that we can also study and understand high-low frequency detectors.
    </p>

    <a class="header-self-link" href="#function">
      <h2 id="function">
        Function
      </h2>
    </a>

    <p>
      It's there in the name, but how sure can we be that &ldquo;high-low frequency detectors&rdquo; are actually detecting what we think they're detecting?
    </p>

    <p>
      Three tools that we can use to investigate their function include <a href="#feature-visualization">feature visualization</a>, <a href="#dataset-examples">dataset examples</a>, and <a href="#tuning-curves">synthetic tuning curves</a>.
    </p>
    <ul>
      <li>
        <b>Feature visualization</b> allows us to establish a causal link between each neuron and its function.<d-cite bibtex-key="erhan2009visualizing,yosinski2015understanding,olah2017feature"></d-cite>
      </li>
      <li>
        <b>Dataset examples</b> show us where the neuron fires in practice.
      </li>
      <li>
        <b>Synthetic tuning curves</b> show us how variation affects the neuron's response.
      </li>
    </ul>

    <p>
      Using these tools, we build up a hypothesis about the function of high-low frequency detectors. Later on in the article, we strengthen this hypothesis by  investigating the mechanical details of their <a href="#implementation">implementation</a> and their <a href="#usage">use</a>.
    </p>

    <a class="header-self-link" href="#feature-visualization">
      <h3 id="feature-visualization">Feature Visualization</h3>
    </a>

    <p>
      A <a href="https://distill.pub/2017/feature-visualization">feature visualization</a> is a synthetic input
      optimized to elicit maximal activation of a single, specific neuron.
      Feature visualizations are constructed starting from random noise, so each and every pixel in a feature visualization
      that's <i>changed</i> from random noise is there because it caused the neuron to activate more strongly. This
      establishes a <a href="https://distill.pub/2020/circuits/zoom-in#claim-1">causal link</a>! The behavior shown in the
      feature visualization causes the neuron to fire:
    </p>

    <style>
      .gallery {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(28px, 64px));
        grid-gap: 0.5rem;
        justify-content: start;
      }

      ul.gallery {
        padding-left: 0;
      }

      .gallery img,
      .gallery-img {
        max-width: 100%;
        width: unset;
        object-fit: none;
        object-position: center;
        border-radius: 8px;
      }

      @media screen and (min-width: 768px) {
        .gallery {
          grid-template-columns: repeat(7, minmax(28px, 96px));
          justify-content: left;
        }
      }
      @media screen and (min-width: 1180px) {
        .gallery {
          grid-gap: 1rem;
        }
      }
    </style>
    <figure id="figure-1">
      <ul class="gallery l-page">
        <% for (const unit of require("../static/diagrams/1.1-feature-vis/units.json").units) {%>
        <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_${unit}.html" style="border-bottom: none; display: flex; flex-direction: column;">
          <img src="<%= `diagrams/1.1-feature-vis/neuron-${unit}.png` %>" title="<%= "Unit " + unit %>" />
          <span class="figcaption"><%= `3a:${unit}`%></span>
        </a>
        <% } %>
      </ul>
      <figcaption class="figcaption l-body">
        <a href="#figure-1" class="figure-number">1</a>:
        Feature visualizations of a variety of high-low frequency detectors from InceptionV1's <a href="https://microscope.openai.com/models/inceptionv1/mixed3a_0?models.op.feature_vis.type=neuron&models.op.technique=feature_vis"><code>mixed3a</code></a> layer.
      </figcaption>
    </figure>

    <p>
      From their feature visualizations, we observe that all of these high-low frequency detectors share these same
      characteristics:
    </p>

    <ul>
      <li>
        <b>Detection of adjacent high and low frequencies.</b> The detectors look for <i>high frequency</i> on one side, and <i>low frequency</i> on the other side.
      </li>
      <li>
        <b>Orientedness.</b> The detectors only detect that high-low frequency change along a particular angle.
      </li>
      <li>
        <b>Together, they span the range of orientations.</b> Just as we observed by creating a <a href="https://distill.pub/2020/circuits/curve-detectors/#joint-tuning-curves">joint tuning curve</a> for curve detectors<d-cite bibtex-key="cammarata2020curve"></d-cite>, the high-low frequency detectors span the full 360º of possible orientations.
      </li>
    </ul>
    <aside>
      <p>By &ldquo;high frequency&rdquo; and &ldquo;low frequency&rdquo; here, we mean <a
          href="https://en.wikipedia.org/wiki/Spatial_frequency">spatial frequency</a>&mdash;just like when we take the
        Fourier transform of an image.</p>
    </aside>

    <p>We can use a <a href="https://distill.pub/2017/feature-visualization/#diversity">diversity term</a> in our feature visualizations to jointly optimize for the activation of a neuron while encouraging different activation patterns in a batch of visualizations. We are thus reasonably confident that if high-low frequency detectors were also sensitive to other patterns, we would see signs of them in these feature visualizations. Instead, the frequency contrast remains an invariant aspect of all these visualizations:</p>

    <figure id="figure-1-2">
      <div class="gallery l-page" style="margin-bottom: 1em;">
        <% for (const unit of [0,1,2,3,4,5,6]) {%>
        <img src="<%= `diagrams/1.1-feature-vis/fv-mixed3a-136-diversity-${unit}.png` %>" title="<%= "Unit mixed3a:136 optimized with a diversity objective."%>" />
        <% } %>
        </div>
      <figcaption class="figcaption l-body">
        <a href="#figure-1-2" class="figure-number">1-2</a>:
        Feature visualizations of high-low frequency detector mixed3a:136 from InceptionV1's <a
          href="https://microscope.openai.com/models/inceptionv1/mixed3a_0?models.op.feature_vis.type=neuron&models.op.technique=feature_vis"><code>mixed3a</code></a>
        layer, optimized with a diversity objective. You can learn more about feature visualization and the diversity objective <a href="https://distill.pub/2017/feature-visualization/#diversity">here</a>.
      </figcaption>
    </figure>

    <a href="#dataset-examples" class="header-self-link">
      <h3 id="dataset-examples">Dataset Examples</h3>
    </a>
    <p>
      Checking against <i>dataset examples</i> helps ensure we're not misreading the feature visualizations. We generate these by finding a natural data distribution (in this case, the training set) and selecting the images that cause the neurons to maximally activate.

    </p>

    <figure id="1.2.0-dataset-examples">
      <div style="display: grid; grid-template-columns: min-content 5fr 1fr; margin-bottom: 1em;">
        <a class="undecorated" href="https://microscope.openai.com/models/inceptionv1/mixed3a_0/136" style="margin-right: 1em;">
          <img src="diagrams/1.1-feature-vis/neuron-136.png" class="gallery-img"
            style="max-width: 65px; border-radius: 8px;" />
        </a>
        <img src="diagrams/1.2-dataset-examples/placeholder.png"/>
      </div>
      <figcaption class="figcaption">
        <a href="#1.2.0-dataset-examples" class="figure-number">2</a>:
        Crops taken from Imagenet<d-cite bibtex-key="imagenet5206848"></d-cite> where <a href="https://microscope.openai.com/models/inceptionv1/mixed3a_0/136"><code>mixed3a</code> 136</a> activated maximally,
        argmaxed over spatial locations.
      </figcaption>
    </figure>

    <p>A wide range of real-world situations can cause these detectors to fire. Oftentimes it's a highly-textured, in-focus foreground object against a blurry background&mdash;the microphone's latticework, the hummingbird's tiny head feathers, or the small rubber dots on a Lenovo ThinkPad <a href="https://en.wikipedia.org/wiki/Pointing_stick">pointing stick</a>&mdash;but not always: we also observe that it fires for the MP3 player's brushed metal finish against its shiny screen, or the text of a watermark.</p>

    <p>In all cases, we see one area with high frequency and another area with low frequency. Even though they sometimes fire at an object boundary, they seem to require that a frequency difference be present (not all objects are so finely textured as that hummingbird!), and they sometimes fire in the complete absence of any object boundaries at all but where frequency difference is present anyways (as with that MP3 player). High-low frequency detectors are therefore not the same as <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">boundary detectors</a>.</p>

    <a href="#tuning-curves" class="header-self-link">
      <h3 id="tuning-curves">Synthetic Tuning Curves</h3>
    </a>

    <p>
      Originally a technique of neuroscience research<d-cite bibtex-key="pmid16993807"></d-cite>, <b>tuning curves</b> measure neural response to a continuous stimulus parameter.
      If high-low frequency detectors are sensitive to orientation, we should expect to be able to demonstrate this via a tuning curve over different orientations, similar to what we showed for <a href="https://distill.pub/2020/circuits/curve-detectors/#radial-tuning-curve">curve detectors</a>.
    </p>

    <p>
      To construct such a curve, we'll need a set of <i>synthetic stimuli</i> which we would expect to cause high-low frequency detectors to fire. In this case, we'll start with images that contain a high-low frequency edge at a known orientation:
    </p>

    <figure id="1.4-tuning-curves-space-axis-1">
      <img src="diagrams/1.4-tuning-curves/orientation.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        The first axis of variation of our synthetic stimuli is <em>orientation</em>.
      </figcaption>
    </figure>

    <p>
      For the curve detector tuning curve we also varied a second parameter, arc length. For
      high-low frequency detectors, we instead vary the ratio between the two frequencies:
    </p>

    <figure id="1.4-tuning-curves-space-axis-2">
      <img src="diagrams/1.4-tuning-curves/ratio.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        The second axis of variation of our synthetic stimuli is the <em>frequency ratio</em>.
        <!--  on the two sides of the high-low frequency edge. -->
      </figcaption>
    </figure>

    <p>These two axes then define a 2-D space from which we can sample synthetic stimuli and measure each neuron's responses to them:</p>

    <d-figure id="figure-3"><figure style="display: grid;"></figure></d-figure>
    <p>
      Each high-low frequency detector exhibits a clear preference for a limited range of orientations. Furthermore, just as we previously found with curve detectors, the family of high-low frequency detectors — even just the six shown here — covers the full 360º space of orientations!
    </p>

    <!-- Alternate title: Upstream Circuits -->
    <!-- Alternate alternate title: Feature Implementation -->
    <a class="header-self-link" href="#implementation">
      <h2 id="implementation">Implementation</h2>
    </a>
    <p>How are high-low frequency detectors built up from lower-level neurons? In particular, where does their oriented nature come from? We see two possibilities, although some mixture is also possible.</p>

    <ul>
      <li>
        <b>Equivariant-to-Equivariant Hypothesis.</b> The first possibility is that the previous layer already has precursor features which detect oriented transitions from high frequency to low frequency. The extreme version of this hypothesis would be that the high-low frequency detector is just an identity passthrough of some lower layer neuron. A more moderate version would be something like what we see with curve detectors, where <a href="https://distill.pub/2020/circuits/early-vision/#group_conv2d2_tiny_curves">early curve detectors</a> become refined into the larger and more sophisticated <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_curves">late curve detectors</a>. Another example would be how edge detection is built up from simple <a href="https://distill.pub/2020/circuits/early-vision/#group_conv2d0_gabor_filters">Gabor filters</a> which were already oriented.

        We call this Equivariant-to-Equivariant because the equivariance over orientation was already there in the previous layer.
      </li>
      <li>
        <b>Invariant-to-Equivariant Hypothesis.</b> Alternatively, previous layers might not have anything like high-low frequency detectors. Instead, the orientation comes from spatial arrangements in the neuron's weights that govern where it is excited by low-frequency and high-frequency features.
      </li>
    </ul>

    <p>To resolve this question, we can look at the weights.</p>

    <p>Glancing at the weights from <code>conv2d2</code> to <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_110.html"><code>mixed3a</code> 110</a>, most of them can be roughly divided into two categories: those that activate on the left and inhibit on the right, and those that do the opposite.
    </p>

    <d-figure id="figure-4">
      <figure >
        <%= require('../static/diagrams/figure-4.svg') %>
        <figcaption id="figure-4-caption">
            <a href="#figure-4" class="figure-number">4</a>:
            Six neurons from conv2d2 contributing weights to mixed3a 110.

            <!--Neurons in the top row <span class="legend-label support-rb">activate</span> on the left and <span class="legend-label inhibit-rb">inhibit</span> on the right; neurons in the bottom row <span class="legend-label inhibit-rb">inhibit</span> on the left and <span class="legend-label support-rb">activate</span> on the right.-->
        </figcaption>
      </figure>
    </d-figure>

    <p>
      The same also holds for each of the other high-low frequency detectors&mdash;but with different spatial patterns on the weights, implementing the different orientations.
    </p>

    <p>
      Surprisingly, across all high-low frequency detectors, the two clusters of neurons that we get for each are actually the <i>same</i> two clusters! One cluster appears to detect textures with a generally high frequency, and one cluster appears to detect textures with a generally low frequency.
    </p>
    <d-figure id="figure-5">
      <figure style=''>
        <!-- <%= require("../static/images/Underlying-Weights.svg") %> -->
        <!-- <img src="images/Underlying-Weights.svg" /> -->
        <!-- <%= require('../static/images/Underlying-Weights.svg') %> -->
        <img src="diagrams/HF-LF-clusters-amd-weight-structure.png" style="width: 100%; max-width: 549px;"/>
        <figcaption>
          <p>
            <a href="#figure-5" class="figure-number">5</a>:
            The strongest weights on any high-low frequency detector (here shown: <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_110.html"><code>mixed3a</code> 110</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_136.html"><code>mixed3a</code> 136</a>, and <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_112.html"><code>mixed3a</code> 112</a>) can be divided into roughly two clusters. Each cluster contributes its weights in similar ways.
          </p>
          <p>
            Top row: underlying neurons <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_119.html"><code>conv2d2</code> 119</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_102.html"><code>conv2d2</code> 102</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_123.html"><code>conv2d2</code> 123</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_90.html"><code>conv2d2</code> 90</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_89.html"><code>conv2d2</code> 89</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_163.html"><code>conv2d2</code> 163</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_98.html"><code>conv2d2</code> 98</a>, and <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_188.html"><code>conv2d2</code> 188</a>.
          </p>
        </figcaption>
      </figure>
    </d-figure>

    <p>
      This is evidence for the Invariant-to-Equivariant hypothesis over the Equivariant-to-Equivariant hypothesis! Let's investigate both parts of that hypothesis in more detail: first, the claim that high-low frequency detectors are built from high frequency detection and low frequency detection, and second, that those two features combine together in a spatially-specific way (arguably, a circuit) in order produce high-low frequency detectors.</p>

    <h3>High and Low Frequency Factors</h3>

    <p>It would be nice if we could confirm that these two clusters of neurons are real. It would also be nice if we could create a simpler way to represent them for circuit analysis later.</p>

    <p>Factorizing the connections<d-footnote>Between two adjacent layers, "connections" reduces to the weights
        between the two layers. Sometimes we are interested in observing connectivity between layers that may not be
        directly adjacent. Because our model, a deep convnet, is non-linear, we will need to approximate the
        connections. A simple approach that we take is to linearize the model by removing the non-linearities. While
        this is not a great approximation of the model's behavior, it does give a reasonable intuition for
        counterfactual influence: had the neurons in the intermediate layer fired, how it would have affected neurons in
        the downstream layers. We treat positive and negative influences separately.</d-footnote> between lower layers and the high-low frequency detectors is one way that we can check whether these two clusters are meaningful and investigate their significance. Performing a non-negative matrix factorization (NMF) to separate the connections into two factors gives us two vectors over neurons.</p>
    <p>Those vectors, in turn, can themselves be fed into feature visualization. Strikingly, the feature visualizations for
      each factor confirm the first half of the Invariant-to-Equivariant hypothesis: one clearly displays a generic high-frequency image, whereas the other does the same with a low-frequency image. We'll call these the <em>HF-factor</em> and the <em>LF-factor</em>:</p>

    <d-figure id="figure-6">
      <%= require('./diagrams/upstream-neurons.ejs')() %>
    </d-figure>

    <p>How can we be more sure that we are interpreting these clusters right? One thing we can do is to create synthetic stimuli again, but targeted at those two NMF factors. This time, we construct a grid that varies both frequency and angle of orientation.</p>

    <d-figure id="figure-7"><figure></figure></d-figure>

    <p>Unlike last time, these activations now mostly ignore the image's orientation, but are sensitive to its frequency. We can average these results over all orientations in order to produce a simple tuning curve of how each factor responds to frequency. As predicted, the HF-factor responds to high frequency and the LF-factor responds to low frequency.</p>

    <d-figure id="figure-8">
      <figure>
        <%= require('../static/diagrams/hf-lf-responses.svg') %>
        <!-- <img src="diagrams/hf-lf-responses.svg" style="margin-bottom: 20px;"/> -->
      <figcaption>
          <p>
            <a href="#figure-8" class="figure-number">8</a>:
            Tuning curve for HF-factor and LF-factor from <code>conv2d2</code> against images with synthetic frequency, averaged across orientation. Wavelength as a proportion of the full input image ranges from 1:1 to 1:10.
          </p>
      </figcaption>
      </figure>
    </d-figure>


    <p>Next, let's finish off the second half of the Invariant-to-Equivariant hypothesis: how are these two factors used to construct high-low frequency detectors?</p>

    <h3>Construction of High-Low Frequency Detectors</h3>

    <p>NMF also recovers the weights between each of the two factors and the high-low frequency detectors. Unsurprisingly, these weights basically reproduce the same behavior that we'd previously been seeing in <a href="#figure-5">Figure 5</a> from the two different clusters of neurons: where the HF-factor inhibits, the LF-factor activates&mdash;and vice versa.</p>

    <%= require('./diagrams/upstream-nmf.ejs')() %>

    <p>High-low frequency detectors are therefore built up by circuits that arrange high frequency detection on one side and low frequency detection on the other, confirming the second half of the Invariant-to-Equivariant hypothesis.</p>

    <p>Against the Equivariant-to-Equivariant hypothesis, note that even texture contrast detectors such as <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_181.html"><code>conv2d2</code> 181</a> seem to fall on one side or the other of this dichotomy. Although <code>conv2d2</code> 181 is most strongly connected to the high-low frequency detector <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3a_70.html"><code>mixed3a</code> 70</a>, the structure of its weights indicates that it contributes more generically to high-frequency detection as a component of the HF-factor rather than contributing any of the existing spatial structure of its texture contrast detection directly.</p>

    <d-figure>
      <figure>
        <img src="diagrams/2d2-181-3a-70-weight.png" style="width: 100%; max-width: 320px; margin: auto; margin-bottom: 20px; display: block; image-rendering: pixelated;"/>
      <figcaption>
          The weights from <code>conv2d2</code> 181 to <code>mixed3a</code> 70 are consistent with <code>conv2d2</code> 181 contributing via the HF-factor, not via the existing spatial structure of its texture contrast detection.
      </figcaption>
      </figure>
    </d-figure>

    <p>Now that we understand how they are constructed, how are high-low frequency detectors used by higher-level features?</p>

    <!-- Alternate title: Downstream Circuits -->
    <!-- Alternate alternate title: Feature Usage -->
    <a class="header-self-link" href="#usage">
      <h2 id="usage">Usage</h2>
    </a>

    <!-- <h3>In mixed3b</h3> -->

    <!-- <d-figure>
      <figure>
        <img src="diagrams/downstream-interpretation.svg" style="margin-bottom: 20px; display: block;"/>
      <figcaption>
          <p>(TODO fix this diagram still!!) Some of <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_314.html"><code>mixed3b</code> 314</a>'s high-low frequency detector weights in more explicit detail.</p>
          <p>One of the functions of high-low frequency detectors appears to be to indicate discontinuities between different objects or regions, as well as an object's continuity with itself.
          </p>
      </figcaption>
      </figure>
    </d-figure> -->

    <p>
      <a href="https://distill.pub/2020/circuits/early-vision/#mixed3b">mixed3b</a> is the next layer immediately after where we see high-low frequency detectors appear. Here, high-low frequency detectors contribute to a variety of features: <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_bumps">bumps</a> and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_divots">divots</a>, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_bar_line_like">line-like</a> and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_curves_misc.">curve-like</a> shapes, boundary detectors,
      and at least one each of <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_281.html">center-surrounds</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_372.html">patterns</a>, and <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_276.html">textures</a>.
    </p>

    <d-figure style="grid-column-end: page-end" id="figure-10">
      <figure>
        <!-- <%= require('../static/diagrams/downstream-circuits.svg') %> -->
        <img src="diagrams/usage-1.png" />
        <!-- <img src="diagrams/downstream-circuits.svg" style="margin-bottom: 20px"/> -->
      <figcaption style="grid-column-end: text-end">
        <p>
            <a href="#figure-10" class="figure-number">10</a>:
            Examples of neurons that high-low frequency detectors contribute to: <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_365.html"><code>mixed3b</code> 365</a> (an hourglass shape detector), <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_276.html"><code>mixed3b</code> 276</a> (a center-surround texture detector), and <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_314.html"><code>mixed3b</code> 314</a> (a double boundary detector).
        </p>
      </figcaption>
      </figure>
    </d-figure>

    <p>
      Oftentimes, downstream features appear to ignore the orientation of a high-low frequency detector, responding roughly the same way to <i>both</i> possible polarities of a given direction&mdash;regardless of which side is high frequency. Consider how <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_345.html"><code>mixed3b</code> 345</a>, the top example above, has its strongest activations from high-low frequency detectors that detect change across a vertically-oriented line. By responding to both, high-low frequency detectors appear to help detect boundaries between different objects or regions, as well as an object's continuity with itself.
    </p>

    <d-figure id="figure-11">
      <figure>
        <img src="diagrams/usage-2.png" style="width: 100%; max-width: 394px; margin-bottom: 20px;"/>
      <figcaption>
        <p>
          <a href="#figure-11" class="figure-number">11</a>:
          Some of <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_314.html"><code>mixed3b</code> 314</a>'s weights, extracted for emphasis. Orientation doesn't matter so much for how these weights are used by <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_314.html"><code>mixed3b</code> 314</a>, but their 180º-invariant orientation does!</p>
        <p>Strong <span class="legend-label support-rb">activation</span> (left) indicates the presence of a <b>boundary</b> at a particular angle, whereas strong <span class="legend-label inhibit-rb">inhibition</span> (right) indicates <b>object continuity</b> where a boundary might otherwise have been.</p>
      </figcaption>
      </figure>
    </d-figure>

    <!-- <%= require('./diagrams/downstream-neurons.ejs')() %> -->

    <p>Given their usefulness for detecting continuities and discontinuities, by far the primary downstream contribution of high-low frequency detectors is to <i>boundary detectors</i>. Of the top 20 neurons in <code>mixed3b</code> with the highest L2-norm of weights across all high-low frequency detectors, eight of those 20 neurons participate in boundary detection of some sort: <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_double_boundary">double boundary detectors</a>, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary_misc">miscellaneous boundary detectors</a>, and especially <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">object boundary detectors</a>. </p>

    <h4>Role in object boundary detection</h4>

    <p><a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">Object boundary detectors</a></i> are neurons which detect boundaries between objects, whether that means the boundary between one object and another or the transition from foreground to background. These detectors are not the same as edge detectors or curve detectors: although they are sensitive to edges (indeed, some of their strongest weights are from lower-level edge detectors!), object boundary detectors are also sensitive to other indicators such as color contrast or high-low frequency, combining together multiple such cues from lower layers.</p>


    <figure id="figure-12">
      <img src="diagrams/usage-boundary.png" />
      <figcaption style="width: 80%; position: absolute; left: 20%; top: 75%;">
        <a href="#figure-12" class="figure-number">12</a>: <code>mixed3b</code> 345 is a boundary detector activated by high-low frequency detectors, edges, color contrasts, and end-of-line
        detectors. It is specifically sensitive to vertically-oriented high-low frequency detectors, regardless of their
        orientation, and along a vertical line of positive weights.
      </figcaption>
    </figure>

    <p> High-low frequency detectors contribute to these object boundary detectors by providing one type of cue that an object has ended and something else has begun. Some examples of object boundary detectors are given below, along with their weights to certain high-low frequency detectors.</p>

    <d-figure id="figure-13">
      <figure>
        <style>
          /* TODO: Optimize smaller breakpoints by hand */
        </style>
        <img src="diagrams/usage-boundaries.png" style="margin-bottom: 20px;"/>
        <figcaption>
          <a href="#figure-13" class="figure-number">13</a>: Four examples of object boundary detectors that high-low frequency detectors contribute to: <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_345.html"><code>mixed3b</code> 345</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_376.html"><code>mixed3b</code> 376</a>, <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_368.html"><code>mixed3b</code> 368</a>, and <a href="https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/mixed3b_151.html"><code>mixed3b</code> 151</a>.
        </figcaption>
      </figure>
    </d-figure>

    <p>Notice how similar the weights are within each of the 180º-invariant orientation groupings! The orientation of the high-low frequency detector and the spatial arrangement of its weights each contribute to the object boundary detector's overall shape.</p>

    <!-- <p class="todo">Show cosine similarity of forward weights as compared to incoming weights; expect -1 incoming; 1 outgoing. Can put this in a table?</p> -->

    <!-- <h3>In mixed4a</h3> -->

    <p>
      Beyond <code>mixed3b</code>, high-low frequency detectors ultimately play a role in detecting more sophisticated object shapes in <code>mixed4a</code> and beyond by continuing to contribute to the detection of boundaries and contiguity.
    </p>

    <!-- <figure id="3.2-downstream-nmf">
      <img src="diagrams/3.2-downstream-nmf/placeholder.png" />
      <figcaption class="figcaption">
        <p>8 neurons from <code>mixed4a</code> (top row) with the highest L2-norm of weights on the given 9 boundary detectors (left column) from <code>mixed3a</code>, along with weights: <span class="legend-label support">activation</span> and <span
        class="legend-label inhibit">inhibition</span>.</p>
      </figcaption>
    </figure> -->

    <p>
    So far, the scope of our investigation has been limited to InceptionV1.

    How common are high-low frequency detectors in convolutional neural networks generally?</p>

    <!--Universality-->
    <h2>Universality</h2>

    <h3>High-Low Frequency Detectors in Other Networks</h3>

    <p>It's always good to ask if what we see is the rule or an interesting exception<d-cite bibtex-key="li2015convergent"></d-cite>&mdash;and high-low frequency detectors seem to be the rule.
    High-low frequency detectors similar to ones in InceptionV1 can be found in a variety of architectures.
    </p>

    <%= require('./diagrams/similar-directions.ejs')() %>

    <p>Notice that these detectors are found at very similar depths within the different networks, between 29% and 33% network depth! While the particular orientations each network's high-low frequency detectors respond to may vary slightly, each network has its own family of detectors that together cover the full 360º range of orientations.
      <d-footnote>Architecture aside – what about networks trained on substantially different <i>datasets</i>? In the extreme case, one could imagine a synthetic dataset where high-low frequency detectors don't arise. For most practical datasets, however, we expect to find them. For example, we even find some candidate high-low frequency detectors in AlexNet (Places): <a href="https://microscope.openai.com/models/alexnet_caffe_places365/conv2_Conv2D_0/37">down-up</a>, <a href="https://microscope.openai.com/models/alexnet_caffe_places365/conv2_Conv2D_0/91">left-right</a>, and <a href="https://microscope.openai.com/models/alexnet_caffe_places365/conv2_Conv2D_0/104">up-down</a>.</d-footnote>
    </p>

    <p>Will we also discover that these detectors are built up from high and low frequency detection, even though they're from three completely different networks?</p>

    <h3>HF-factor and LF-factor in Other Networks</h3>

    <p>
      As we did with InceptionV1, we can again perform NMF on the weights of the high-low frequency detectors in each network in order to extract the strongest two factors.
    </p>

    <%= require('./diagrams/universality.ejs')() %>


    <p>The feature visualizations of the two factors reveal one clear HF-factor and one clear LF-factor, just like what we found in InceptionV1. Furthermore, the weights on the two factors are again very close to symmetric.</p>
    <p>Our earlier conclusions about the Invariant-to-Equivariant hypothesis therefore also hold across these different networks: high-low frequency detectors are built up from placing high frequency detection and low frequency detection in a specific spatial arrangement.</p>

    <h2>Conclusion</h2>

    <p>Although high-low frequency detectors represent a feature that we didn't necessarily expect to find in a neural network, we find that we can still explore and understand<d-cite bibtex-key="karpathy2015visualizing"></d-cite> them using the interpretability tools we've built up for exploring <a href="https://distill.pub/2020/circuits/zoom-in/">circuits</a>: NMF, feature visualization, synthetic stimuli, and more.

    <p>We've also learned that high-low frequency detectors are an important part of later, higher-level features, are built up from comprehensible lower-level parts, and are common across multiple network architectures.
    </p>

    <p>Despite their surprisingness, we wonder whether their existence isn't so unnatural after all: as seen previously in <a href="#dataset-examples">the dataset examples</a>, a high-low frequency change can indicate the border between an in-focus foreground object and its more blurry background (or between an in-focus background and the more blurry foreground, or even between two objects at different focuses).</a> Changes such as these are something of a natural concept: the aesthetic quality imparted by the blurriness of an out-of-focus region of an image is known as <a href="https://en.wikipedia.org/wiki/Bokeh"><i>bokeh</i></a> in photography, and getting it right as a depth cue is one of the notable challenges explored by developers of immersive VR.</p>

    <p>Depth perception is simply one of the challenges that's endemic to vision, and frequency change is one cue that a vision system can use for getting it right. Just as neuroscientists have already uncovered and characterized curve detectors, the authors therefore wonder whether high-low frequency detectors might also be lurking in that original neural network vision system, the animal visual cortex.</p>




  </d-article>

  <d-appendix>
    <h3>Author Contributions</h3>

    <p>As with many scientific collaborations, the contributions to the high-low frequency detectors paper are difficult to separate because it was a collaborative effort that we wrote together.</p>

    <p><b>Conceptual Contributions.</b> Christopher Olah originally noted the high-low frequency directors as a research direction. </p>

    <p><b>Experiments.</b> Ludwig Schubert wrote the code for generating and measuring synthetic tuning curves for the high-low frequency detectors, for performing NMF on the high-low frequency detectors to extract the HF-factor and the LF-factor, and for performing NMF on the high-low frequency detectors from other networks. Chelsea Voss wrote the code for generating and measuring synthetic stimuli for the HF-factor and LF-factor, with help from Chris for extracting and using the NMF components.</p>

    <p><b>Figures.</b> Ludwig designed the visualization of the high-low frequency detector synthetic tuning curves, the visualization of the HF-factor and LF-factor NMF vectors, the visualization of the HF-factor and LF-factor NMF weights from conv2d1 and conv2d2, and the figures demonstrating high-low frequency detectors from other networks shown in the Universality section. Chelsea designed the visualization of the HF-factor and LF-factor synthetic stimuli results and the figures articulating the downstream use of high-low frequency detectors, and edited some of the final figures. Two figures were borrowed from Zoom In and Early Vision.

    <p><b>Writing.</b> Chelsea and Ludwig wrote the paper, with feedback from Chris.</p>

    <!--Potential Limitations-->
    <!-- <h3>Limitations</h3>
    <h4>How bad is the linearization?</h4>
    <p class="todo">Maybe we can measure and put up some <em>summary statistics</em> on how bad our linear approximation
      really is.
      Maybe in terms of % variance of resulting activation? -> colah if there's sth more principled.</p>

    <figure id="5.1-linearization-badness">
      <img src="diagrams/5.1-linearization-badness/placeholder.jpg" />
      <figcaption class="figcaption">
        Maybe show % of variance explained over distance between layers or sth; if so likely move to appendix.
      </figcaption>
    </figure> -->


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
