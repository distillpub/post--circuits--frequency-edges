<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1><span style="display: inline; color: gray; margin-right: 0.25em">2.4</span>High-Low Frequency Detectors</h1>
    <p>A family of early-vision filters reacting to contrasts between spatial gratings of different frequency</p>
    <figure id="hero" class="l-body todo">I'm just a figure who's a hero for fun.</figure>
  </d-title>

  <d-article>

    <h2 style="display: none;">Introduction</h2>
    <p>
      Why should we study the awkwardly-named "high-low frequency detectors" when we've just walked you through <a href="https://distill.pub/2020/circuits/curve-detectors/">curve
      detectors</a> in detail? There's a strategic argument from the author's side: everybodyÂ® would guess that vision
      networks contain curve detectors. It's easy to imagine how they are built up from earlier edge detectors, and
      they're common enough that neuroscientists routinely find them in animal visual cortices. "High-low frequency
      detectors", on the other hand, form a family of features we did not expect to find. Still, they have many of the
      hallmarks of important features: their existence feels natural in retrospect, they're an important part of later,
      higher-level features, and they're common across network architectures.
    </p>


    <!--Empirical observation-->
    <h2>Empirical Observation</h2>

    <h4>Feature Visualization</h4>
    <p>We originally found high-low frequency detectors by studying models' neurons' <a
        href="//distill.pub/2017/feature-visualization">feature visualizations</a>: synthetic inputs
      optimized to elicit maximal activation of a single, specific neuron.</p>
    <style>
      .gallery {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(28px, 56px));
        grid-gap: 0.5rem;
        justify-content: start;
      }

      ul.gallery {
        padding-left: 0;
      }

      .gallery img {
        max-width: 100%;
        width: unset;
        object-fit: none;
        object-position: center;
        border-radius: 0.25rem;
      }

      @media screen and (min-width: 768px) {
        .gallery {
          grid-template-columns: repeat(7, minmax(28px, 112px));
          grid-gap: 1rem;
          justify-content: center;
        }
      }
    </style>
    <figure class="l-screen base-grid" id="1.1-feature-vis">
      <ul class="gallery l-page">
        <% for (const unit of require("../static/diagrams/1.1-feature-vis/units.json").units) {%>
        <img src="<%= `diagrams/1.1-feature-vis/neuron-${unit}.png` %>" title="<%= "Unit " + unit %>" />
        <% } %>
      </ul>
      <figcaption class="figcaption l-body">
        <a href="#1.1-feature-vis" class="figure-number">1</a>:
        A variety of detectors from InceptionV1's <a href="https://microscope.openai.com/models/inceptionv1/mixed3a_0?models.op.feature_vis.type=neuron&models.op.technique=feature_vis"><code>mixed3a</code></a> layer.
      </figcaption>
    </figure>

    <h4 id="dataset-examples">Dataset Examples</h4>
    <p>
      To ensure we're not misreading those feature visualizations it's good practice to check what such filters
      maximally activate on over a natural data distribution, say the training set.
    </p>

    <figure id="1.2-dataset-examples">
      <img src="diagrams/1.2-dataset-examples/placeholder.png" />
      <figcaption class="figcaption">
        <a href="#1.2-dataset-examples" class="figure-number">2</a>:
        Crops taken from training dataset, Imagenet, on which a high-low frequency detector activated maximally;
        argmaxed over spatial locations.
      </figcaption>
    </figure>

    <p>
      The dataset examples show that a wide range of real-world objects can cause these detectors to fire: deep 3D
      textures, such as on the microphone against a blurred background, but also surface textures such as the MP3
      player's metal finish against its shiny screen.
    </p>

    <h4>Tuning Curves</h4>
    <p>
      In the curve detector chapter we saw an example of a <a href="https://distill.pub/2020/circuits/curve-detectors/#radial-tuning-curve">tuning curve</a>: it plots filter response over rotation angle,
      where the input is a natural image that we know has curvature in it. We can repeat this experiment for high-low
      frequency detectors: since their response is also specific to an angle, we expect to see a similar pattern of
      angle-specific activation. To measure this, we create artificial stimuli: images that contain a high-low frequency
      edge at a known orientation:
    </p>

    <figure id="1.4-tuning-curves-space-axis-1">
      <img src="diagrams/1.4-tuning-curves/orientation.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        The first axis of variation of our artificial stimuli is <em>orientation</em>.
      </figcaption>
    </figure>

    <p>
      The curve detector tuning curve also varied a second parameter, arc length. For
      high-low frequency detectors, we will instead vary the ratio between the two frequencies:
    </p>

    <figure id="1.4-tuning-curves-space-axis-2">
      <img src="diagrams/1.4-tuning-curves/ratio.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        The second axis of variation of our artificial stimuli is the <em>ratio between frequencies</em>.
        <!--  on the two sides of the high-low frequency edge. -->
      </figcaption>
    </figure>

    <p>These two axes define a space from which we can sample image stimuli: every 2D point corresponds to a combination
      of
      the two varied attributes. In the tuning curve diagram below, each unit is associated with a 60 by 360 plot of
      activation strength. In those plots, each pixel corresponds to one of the artificial stimuli.</p>

    <!-- <figure id="1.4-tuning-curves-samples">
      <img src="diagrams/1.4-tuning-curves/samples.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        TODO:
      </figcaption>
    </figure> -->

    <!-- <p>
      But high-low frequency detectors seem to have more knobs to turn than just angle: the frequencies of the two
      gratings that make up the frequency edge should also affect the filters' responses. Let's measure activations over
      a space that varies both of those, as well as orientation: Angle by frequency 1 by frequency 2. We will then have
      to visualize this volume in various ways to show it on this 2D screen.
    </p> -->

    <p class="todo done">Legend @shan; smarter ideas? No responsiveness? etc.</p>

    <d-figure id="1.4-tuning-curves">
      <figure></figure>
    </d-figure>

    <p>
      By argmaxing over the two frequencies we recover the angle-response curve from the previous section:
    </p>

    <span class="todo">Radial tuning curve for HLF detectors here?</span>

    <!--Upstream Circuits-->
    <h2>Upstream Circuits</h2>
    <p>How are high-low frequency detectors constructed from lower-level features?</p>

    <p>A natural guess would be to assume these filters combine low frequency detectors and high frequency detectors.
      Let us factorize the connections<d-footnote>Between two adjacent layers, "connections" reduces to the weights
        between the two layers. Sometimes we are interested in observing connectivity between layers that may not be
        directly adjacent. Because our model, a deep convnet, is non-linear, we will need to approximate the
        connections. A simple approach that we take is to linearize the model by removing the non-linearities. While
        this is not a great approximation of the model's behavior, it does give a reasonable intuition for
        counterfactual influence: had the neurons in the intermediate layer fired, how it would have affected neurons in
        the downstream layers. We treat positive and negative influences separately.</d-footnote> between lower level
      features and the high-low frequency detectors we already found:</p>

    <p>NMFing the weights into 2 factors recovers low/high frequency neuron clusters.</p>

    <p class="todo">Fix alignment of leftmost column?</p>

    <%= require('./diagrams/upstream-nmf.ejs')() %>

    <p class="todo"> talk more explicitly about above diagram</p>

    <h4>Underlying Neurons title tbd</h4>

    <p>Let's inspect which lower-level neurons primarily contribute to those NMF factors:</p>

    <%= require('./diagrams/upstream-neurons.ejs')() %>

    <h4>Circuit diagram</h4>

    <p>To understand how these detectors are implemented in terms of even earlier features, we can look at which
      features feed into a given high-low frequency detector, and even at the spatial structure of the weights
      connecting those features to the detector in question.</p>

    <figure id="2.1-upstream-circuit">
      <img src="diagrams/3.1-downstream-circuit/placeholder.png" />
      <figcaption class="figcaption">
        TODO: replace with Nick's interactive version on integration?
      </figcaption>
    </figure>

    <h4>Joint NMF of weights</h4>



    <!--Downstream Circuits-->
    <h2>Downstream Circuits</h2>
    <p>How are high-low frequency detectors used in the construction of higher-level features?</p>


    <h3>In mixed3b</h3>
    <%= require('./diagrams/downstream-neurons.ejs')() %>

    <p>
      As shown above, in <a href="https://distill.pub/2020/circuits/early-vision/#mixed3b">mixed3b</a> high-low frequency detectors contribute to a variety of features: <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_bumps">bumps</a> and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_divots">divots</a>, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_bar_line_like">line-like</a> and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_curves_misc.">curve-like</a> shapes,
      and at least one each of <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_281.html">center-surrounds</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_372.html">patterns</a>, and <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_276.html">textures</a>.
    </p>

    <p>However, by far the primary downstream contribution of high-low frequency detectors is to <i>boundary detectors</i>: to some extent to <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_double_boundary">double boundary detectors</a> and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary_misc">miscellaneous boundary detectors</a>, but especially to <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">object boundary detectors</a>.</p>


    <h4>Role in boundary detection</h4>

    <p><a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">Object boundary detectors</a></i> are neurons which detect boundaries between objects, whether that means the boundary between one object and another or the transition from foreground to background. These detectors are not the same as edge detectors or curve detectors: although they are sensitive to edges (indeed, some of their strongest weights are from lower-level edge detectors!), object boundary detectors are also sensitive to other indicators such as color contrast or high-low frequency, combining together multiple such cues from lower layers.</p>

    <span class="todo">TODO(csvoss) Include JS snippet to autolink these neurons.</span>

    <figure style='image-rendering: pixelated;'>
      <img src="images/Boundary.svg" />
    </figure>

    <p> High-low frequency detectors contribute to these object boundary detectors by providing another type of cue that an object has ended and something else has begun: as seen in <a href="#dataset-examples">the dataset examples</a>, a high-low frequency change can indicate the border between an in-focus foreground object and its more blurry background (or between an in-focus background and the more blurry foreground, or even between two objects at different focuses).</a>

    <p>Object boundary detectors are sensitive to the rotation of the high-low frequency detectors that activate them. Consider how <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_345.html"><code>mixed3b</code> 345</a>, a vertical boundary detector, is activated by only high-low frequency detectors that detect change across a vertically-oriented line.</p>

    <p>These detectors are, however, invariant to the <i>&ldquo;sidedness&rdquo;</i> of the high-low frequency detectors that activate them. Among the high-low frequency detectors that activate <code>mixed3b</code> 345, the high-frequency side may either be on the left (as with <code>mixed3a</code> <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_110.html">110</a>) or on the right (as with <code>mixed3a</code> <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_112.html">112</a>). However, in each case the weights are approximately the same.</p>

    <p class="todo">Show cosine similarity of forward weights as compared to incoming weights; expect -1 incoming; 1 outgoing. Can put this in a table?</p>

    <h3>In mixed4a</h3>

    <p>
      High-low frequency detectors ultimately play a role in detecting more sophisticated object shapes in <code>mixed4a</code> and beyond by contributing to the detection of boundaries and contiguity.
    </p>

    <figure id="3.2-downstream-nmf">
      <img src="diagrams/3.2-downstream-nmf/placeholder.png" />
      <figcaption class="figcaption">
        <p>8 neurons from <code>mixed4a</code> (top row) with the highest L2 norm of weights on the given 9 boundary detectors (left column) from <code>mixed3a</code>.</p>
        <p>Orange indicates activation. Where a neuron is activated by a high-low frequency detector in some direction, the object typically has a <i>boundary</i> orthogonal to that direction.</p>
        <p>Blue indicates inhibition. Where a neuron is inhibited by a high-low frequency detector in some direction, the object typically has <i>contiguity</i> in that direction.</p>
      </figcaption>
    </figure>

    <!--Universality-->
    <h2>Universality</h2>
    <p>How common are high-low frequency detectors in convolutional neural networks?</p>
    <p>Always good to ask if what you see is the rule or an interesting exception. high-low frequency detectors seem to
      be the rule.</p>

    <h4>High-Low Frequency Detectors in Other Models</h4>

    <span class="todo">Hyperlink these</span>

    <span class="todo">4 diagrams mimicking upstream-nmf.ejs?</span>

    <%= require('./diagrams/similar-directions.ejs')() %>

    <h4>Tuning curves in other models</h4>
    <p>Show tuning curves!</p>

    <figure id="4.2-similar-detectors">
      <img src="diagrams/4.2-similar-detectors/placeholder-1.jpg" />
      <figcaption class="figcaption">
        Top k filters responding to stimuli from earlier section
      </figcaption>
    </figure>

    <h4>Re-occurence at deeper layers at different scale</h4>
    <p>Show re-occurence at higher layers at different scales?</p>

    <figure id="4.3-higher-layers">
      <img src="diagrams/4.3-higher-layers/placeholder.png" />
      <figcaption class="figcaption">
        Top k filters responding to stimuli from earlier section. Compare their response to earlier layer hilo
        detectors; ideally show frequency gap / non-huge-overlap in response 2D tuning curve.
      </figcaption>
    </figure>


    <h2>Conclusion</h2>
    <p>?</p>
    <p class="todo">Maybe devote a paragraph to concretizing the "their existence feels natural in retrospect" aspect of high-low frequency detectors â in particular, pointing out that high-low frequency is one way to infer the distance of objects? IIRC @colah has described this to me as such before.</p>

  </d-article>

  <d-appendix>
    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p>

    <!--Potential Limitations-->
    <h3>Limitations</h3>
    <h4>How bad is the linearization?</h4>
    <p class="todo">Maybe we can measure and put up some <em>summary statistics</em> on how bad our linear approximation
      really is.
      Maybe in terms of % variance of resulting activation? -> colah if there's sth more principled.</p>

    <figure id="5.1-linearization-badness">
      <img src="diagrams/5.1-linearization-badness/placeholder.jpg" />
      <figcaption class="figcaption">
        Maybe show % of variance explained over distance between layers or sth; if so likely move to appendix.
      </figcaption>
    </figure>


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
