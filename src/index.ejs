<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1><span style="display: inline; color: gray; margin-right: 0.25em">2.4</span>High-Low Frequency Detectors</h1>
    <p>A family of early-vision filters reacting to contrasts between spatial gratings of different frequency</p>
    <!-- <figure id="hero" class="l-body todo">I'm just a figure who's a hero for fun.</figure> -->
  </d-title>

  <d-article>

    <h2 style="display: none;">Introduction</h2>
    <p>
      Why should we study the awkwardly-named &ldquo;high-low frequency detectors&rdquo; when we've just walked you through <a href="https://distill.pub/2020/circuits/curve-detectors/">curve
      detectors</a> in detail? There's a strategic argument from the author's side: anyone would guess that vision
      networks contain curve detectors. It's easy to imagine how curve detectors are built up from earlier edge detectors, and
      they're common enough that neuroscientists routinely find them in animal visual cortices. High-low frequency
      detectors, on the other hand, form a family of features we did not expect to find. Still, they have many of the
      hallmarks of important features: their existence feels natural in retrospect, they're an important part of later,
      higher-level features, and they're common across network architectures.
    </p>


    <!--Empirical observation-->
    <h2>Empirical Observation</h2>

    <h4>Feature Visualization</h4>
    <p>We originally find high-low frequency detectors by studying <a
        href="//distill.pub/2017/feature-visualization">feature visualizations</a>: synthetic inputs
      optimized to elicit maximal activation of a single, specific neuron.
    </p>
    <p>
      Having <a href="https://distill.pub/2020/circuits/early-vision/">systematically grouped</a> the neurons in the first five layers of InceptionV1, we discover <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3a_high_low_frequency">a group of 15 similar neurons</a> in <code>mixed3a</code> whose feature visualizations all show a clear separation between a <i>high-frequency</i> region and a <i>low-frequency</i> region.
    </p>
    <p>
      We have dubbed this group the <i>high-low frequency detectors</i>.
    </p>
    <style>
      .gallery {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(28px, 65px));
        grid-gap: 0.5rem;
        justify-content: start;
      }

      ul.gallery {
        padding-left: 0;
      }

      .gallery img {
        max-width: 100%;
        width: unset;
        object-fit: none;
        object-position: center;
        border-radius: 0.25rem;
      }

      @media screen and (min-width: 768px) {
        .gallery {
          grid-template-columns: repeat(7, minmax(28px, 65px));
          grid-gap: 1rem;
          justify-content: center;
        }
      }
    </style>
    <figure class="l-screen base-grid" id="1.1-feature-vis">
      <ul class="gallery l-page">
        <% for (const unit of require("../static/diagrams/1.1-feature-vis/units.json").units) {%>
        <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_${unit}.html" style="border-bottom: none; display: flex;">
          <img src="<%= `diagrams/1.1-feature-vis/neuron-${unit}.png` %>" title="<%= "Unit " + unit %>" />
        </a>
        <% } %>
      </ul>
      <figcaption class="figcaption l-body">
        <a href="#1.1-feature-vis" class="figure-number">1</a>:
        A variety of high-low frequency detectors from InceptionV1's <a href="https://microscope.openai.com/models/inceptionv1/mixed3a_0?models.op.feature_vis.type=neuron&models.op.technique=feature_vis"><code>mixed3a</code></a> layer.
      </figcaption>
    </figure>

    <p>By &ldquo;high frequency&rdquo; and &ldquo;low frequency&rdquo; here, we mean &ldquo;frequency&rdquo; in the same way as is used when discussing Fourier transforms of images.
    </p>

    <p>
      Based on these feature visualizations, we first hypothesize that high-low frequency detectors appear to sense where and how an image has changed from a high-frequency regime to a lower-frequency regime. While feature visualization does directly establish a <a href="https://distill.pub/2020/circuits/zoom-in#claim-1">causal link</a> with the stimuli that most activate a neuron, we also have a <a href="https://distill.pub/2020/circuits/zoom-in#claim-1">few other tools</a> in our toolkit in order to investigate the purpose of these neurons more thoroughly, including analyzing <a href="#dataset-examples">dataset examples</a>, <a href="#tuning-curves">tuning curves</a>, <a href="feature-implementation">feature implementation</a>, and <a href="feature-use">feature use</a>.
    </p>

    <h4 id="dataset-examples">Dataset Examples</h4>
    <p>
      Which images from a natural data distribution – say, perhaps, the training set – cause those neurons to maximally activate?
      Checking against <i>dataset examples</i> such as these helps ensure we're not misreading the feature visualizations.
    </p>

    <figure id="1.2-dataset-examples">
      <img src="diagrams/1.2-dataset-examples/placeholder.png" />
      <figcaption class="figcaption">
        <a href="#1.2-dataset-examples" class="figure-number">2</a>:
        Crops taken from training dataset, Imagenet, on which a high-low frequency detector activated maximally;
        argmaxed over spatial locations.
      </figcaption>
    </figure>

    <p>
      These dataset examples demonstrate that a wide range of real-world objects can cause these detectors to fire: deep 3-D
      textures, such as on the microphone against a blurred background, but also surface textures such as the MP3
      player's metal finish against its shiny screen.
    </p>

    <h4 id="tuning-curves">Synthetic Tuning Curves</h4>
    <p>
      In the <a href="https://distill.pub/2020/circuits/curve-detectors">Curve Detectors</a> we give an example of a <a href="https://distill.pub/2020/circuits/curve-detectors/#radial-tuning-curve">tuning curve</a>, plotting filter response over rotation angle,
      where the input is a natural image that we know has curvature in it. We can repeat this experiment for high-low
      frequency detectors. Since their response is also specific to an angle, we should expect to see a similar pattern of
      angle-specific activation. To construct a curve and measure activations, we create <i>synthetic stimuli</i>, in this case images with a high-low frequency
      edge at a known orientation.
    </p>

    <figure id="1.4-tuning-curves-space-axis-1">
      <img src="diagrams/1.4-tuning-curves/orientation.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        The first axis of variation of our artificial stimuli is <em>orientation</em>.
      </figcaption>
    </figure>

    <p>
      The curve detector tuning curve also varies a second parameter, arc length. For
      high-low frequency detectors, we instead vary the ratio between the two frequencies.
    </p>

    <figure id="1.4-tuning-curves-space-axis-2">
      <img src="diagrams/1.4-tuning-curves/ratio.png"
        style="width: 100%; max-width: 636px; image-rendering: pixelated;" />
      <figcaption class="figcaption">
        The second axis of variation of our artificial stimuli is the <em>frequency ratio</em>.
        <!--  on the two sides of the high-low frequency edge. -->
      </figcaption>
    </figure>

    <p>These two axes then define a space from which we can sample synthetic stimuli. Every 2-D point corresponds to a combination
      of
      the two varied attributes. In the diagram below, we give a 60px by 360px heatmap of
      activation strength for each high-low frequency detector, where each pixel in the heatmap corresponds to the detector's response to one of the synthetic stimuli.</p>

    <d-figure id="1.4-tuning-curves">
      <figure></figure>
    </d-figure>
    <p>
      As demonstrated above, each high-low frequency detector exhibits a clear sensitivity to a limited range of orientations. Within those orientations, the detector is excited by a wide range of frequency ratios, some more strongly than others.
    </p>
    <p>
      Just as we saw <a href="https://distill.pub/2020/circuits/curve-detectors/#synthetic-curves">when investigating curve detectors</a>, the synthetic stimuli that cause the strongest activations are the ones that are most similar to the feature visualization – in this case, the ones that are similar along our two axes of orientation and ratio between frequencies!
    </p>

    <!-- Alternate title: Upstream Circuits -->
    <!-- Alternate alternate title: Feature Implementation -->
    <h2 id="feature-implementation">How high-low frequency detectors are implemented</h2>
    <p>How are these specialized high-low frequency detectors constructed from upstream neurons?</p>

    <p>One natural guess (Hypothesis 1) might be to assume these detectors combine together low frequency detection and high frequency detection, placing these two subfeatures in a specific spatial arrangement in order to produce high-low frequency detection. This would be comparable to how InceptionV1 <a href="https://distill.pub/2020/circuits/zoom-in/#claim-2-superposition">constructs a car detector</a> by looking for windows on top, a car body in the middle, and wheels on the bottom.
    </p>

    <p>An alternative guess (Hypothesis 2) might be that these detectors are built up from lower-level features that are roughly similar to high-low frequency detectors, but are more primitive and less reliable. An analogy would be how edge detection is built up from simple <a href="https://distill.pub/2020/circuits/early-vision/#group_conv2d0_gabor_filters">Gabor filters</a>.

    <p>When we inspect the weights of an individual high-low frequency detector such as <code>mixed3a</code> 110, we find that the truth is more in line with Hypothesis 1 than Hypothesis 2. The <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_110.html">weights</a> of <code>mixed3a</code> 110 can be roughly divided into two categories: those that inhibit on the right and activate on the left, and those that do the opposite. (See top row of <a href="#2.1-underlying-weights">Figure 4</a>.)
    </p>

    <p>A similar observation seems to hold for the other high-low frequency detectors as well, though with different spatial orientations: each detector's weights can be divided into roughly two categories. Furthermore, across the high-low frequency detectors, these two categories seem to be comprised of the same neurons. We postulate that one cluster consists of low frequency detecting units, and the other cluster consists of high frequency detecting units.</p>

    <d-figure id="2.1-underlying-weights">
      <figure style=''>
        <img src="images/Underlying-Weights.png" />
        <figcaption>
          <p>
            <a href="#2.1-underlying-weights" class="figure-number">4</a>:
            The strongest weights on any high-low frequency detector (here shown: <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_110.html"><code>mixed3a</code> 10</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_136.html"><code>mixed3a</code> 36</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_112.html"><code>mixed3a</code> 12</a>, and <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_88.html"><code>mixed3a</code> 88</a>) can be divided into roughly two categorical groups.
          </p>
          <p>
            Shown: the weights from neurons <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/conv2d2_119.html"><code>conv2d2</code> 119</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/conv2d2_102.html"><code>conv2d2</code> 102</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/conv2d2_123.html"><code>conv2d2</code> 123</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/conv2d2_90.html"><code>conv2d2</code> 90</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/conv2d2_89.html"><code>conv2d2</code> 89</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/conv2d2_163.html"><code>conv2d2</code> 163</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/conv2d2_98.html"><code>conv2d2</code> 98</a>, and <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/conv2d2_188.html"><code>conv2d2</code> 188</a>.
          </p>
        </figcaption>
      </figure>
    </d-figure>

    <p>Let's investigate both parts of Hypothesis 1 in more detail: first, the claim that high-low frequency detectors are built from high frequency detection and low frequency detection as subfeatures, and second, that those two subfeatures are combined in a spatially-specific way in order to produce high-low frequency detectors.</p>

    <h4>Underlying Factors</h4>

    <p>We have observed two distinct categories of neurons that contribute to the weights of high-low frequency detectors, and speculated that the two categories represent high frequency detection and low frequency detection. It would be nice if we could both confirm this subjective observation mathematically and create a simpler way to represent the two categories for our analyses.</p>

    <p>Factorizing the connections<d-footnote>Between two adjacent layers, "connections" reduces to the weights
        between the two layers. Sometimes we are interested in observing connectivity between layers that may not be
        directly adjacent. Because our model, a deep convnet, is non-linear, we will need to approximate the
        connections. A simple approach that we take is to linearize the model by removing the non-linearities. While
        this is not a great approximation of the model's behavior, it does give a reasonable intuition for
        counterfactual influence: had the neurons in the intermediate layer fired, how it would have affected neurons in
        the downstream layers. We treat positive and negative influences separately.</d-footnote> between lower layers and the high-low frequency detectors is one way that we can check whether these two categories are meaningful and investigate their significance. Performing non-negative matrix factorization (NMF) in each layer to separate the weights into two factors yields the results shown below.</p>

    <d-figure id="2.2-upstream-neurons">
      <%= require('./diagrams/upstream-neurons.ejs')() %>
    </d-figure>

    <p>Strikingly, the feature visualizations for each factor confirm our guess about the functionality of the two categories: one (here named the <i>HF-factor</i>) clearly displays a high-frequency image, whereas the other (here named the <i>LF-factor</i>) clearly displays a low-frequency image. This confirms the first half of Hypothesis 1: these are meaningful categories.</p>

    <p>To further corroborate our guess about the function of these factors, we can again create <i>synthetic stimuli</i>, this time targeted at the two NMF factors. We construct a grid that varies both the frequency of a sinusoidal checkerboard input as well as its angle of orientation, and measure how much the synthetic image activates each factor.</p>

    <d-figure>
      <figure>
        <img src="diagrams/hf-lf-stimuli.svg" style="max-width: 400px; margin-bottom: 20px"/>
      <figcaption>
          <p>
            Frequency and angle of orientation were varied to construct a grid of synthetic stimuli. Next to each factor is a heatmap of its activations in response to each stimulus image, ranging from 0 to 1.
          </p>
      </figcaption>
      </figure>
    </d-figure>


    <p>The activations are relatively independent of the image's angle, but sensitive to its frequency. Averaging the results over all angles produces a line plot of each factor's magnitude of activation by frequency, demonstrating each factor's approximate specificity to a particular range of frequencies.

    <d-figure>
      <figure>
        <img src="diagrams/hf-lf-responses.svg" style="margin-bottom: 20px;"/>
      <figcaption>
          <p>
            Tuning curve for the HF-factor and LF-factor from <code>conv2d2</code> against synthetic high or low frequency images.

            Activations were averaged over angle to produce this plot.
          </p>
      </figcaption>
      </figure>
    </d-figure>


    <p>Instead of dealing with two sets of neurons, in future analyses we can now deal with two <i>vectors</i> over the neurons in <code>conv2d2</code> or <code>conv2d1</code>. Thus equipped with the HF-factor and the LF-factor, we can ask: how are these two factors used to produce high-low frequency detectors?</p>

    <h4>Construction of high-low frequency detectors</h4>

    <p>NMF also recovers the weights between each of the two factors and the high-low frequency detectors, demonstrating how those detectors actually use the HF-factor and the LF-factor.
    Each high-low frequency detector has a region where the HF-factor activates and the LF-factor inhibits, and an opposite region where the LF-factor activates and the HF-factor inhibits.</p>

    <%= require('./diagrams/upstream-nmf.ejs')() %>

    <p>Inspecting these weights shows that high-low frequency detectors are built up by spatial arrangement of high frequency detection and low frequency detection, confirming the second half of Hypothesis 1.</p>

    <p>Against Hypothesis 2, note that even texture contrast detectors such as <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/conv2d2_181.html"><code>conv2d2</code> 181</a> seem to fall on one side or the other of this dichotomy. Although <code>conv2d2</code> 181 is most strongly connected to the high-low frequency detector <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_70.html"><code>mixed3a</code> 70</a>, the structure of its weights indicates that it contributes more generically to high-frequency detection as a component of the HF-factor rather than contributing any of the existing spatial structure of its texture contrast detection directly.</p>

    <d-figure>
      <figure>
        <img src="images/texture-contrast-detector.svg" style="width: 50%; margin: auto; margin-bottom: 20px; display: block"/>
      <figcaption>
          <p>
            The weights from <code>conv2d2</code> 181 to <code>mixed3a</code> 70 are consistent with <code>conv2d2</code> 181 contributing via the HF-factor, not via the existing spatial structure of its texture contrast detection.
          </p>
      </figcaption>
      </figure>
    </d-figure>

    <p>Now that we understand how they are constructed, how are high-low frequency detectors used by higher-level features?</p>

    <!-- Alternate title: Downstream Circuits -->
    <!-- Alternate alternate title: Feature Usage -->
    <h2 id="feature-use">How high-low frequency detectors are used</h2>

    <p>
      In <a href="https://distill.pub/2020/circuits/early-vision/#mixed3b">mixed3b</a>, high-low frequency detectors contribute to a variety of features: <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_bumps">bumps</a> and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_divots">divots</a>, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_bar_line_like">line-like</a> and <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_curves_misc.">curve-like</a> shapes,
      and at least one each of <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_281.html">center-surrounds</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_372.html">patterns</a>, and <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_276.html">textures</a>.
    </p>

    <d-figure>
      <figure>
        <img src="diagrams/downstream-circuits.svg" style="margin-bottom: 20px"/>
      <figcaption>
          <p>
            Examples of neurons that high-low frequency detectors contribute to: <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_365.html"><code>mixed3b</code> 365</a> (an hourglass shape detector), <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_276.html"><code>mixed3b</code> 276</a> (a center-surround texture detector), and <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_314.html"><code>mixed3b</code> 314</a> (a double boundary detector).
          </p>
      </figcaption>
      </figure>
    </d-figure>

    <p>
      In those examples, notice how the weights are used. Oftentimes, downstream features also ignore the &ldquo;sidedness&rdquo; of a high-low frequency detector, responding roughly the same way to both possible polarities of a given direction. This can then used to detect the presence or absence of boundaries or discontinuities along the direction the two detectors share.
    </p>

    <!-- <%= require('./diagrams/downstream-neurons.ejs')() %> -->

    <d-figure>
      <figure>
        <img src="diagrams/downstream-interpretation.svg" style="margin-bottom: 20px; display: block;"/>
      <figcaption>
          <p>Some of <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_314.html"><code>mixed3b</code> 314</a>'s high-low frequency detector weights in more explicit detail.</p>
          <p>One of the functions of high-low frequency detectors appears to be to indicate discontinuities between different objects or regions, as well as an object's continuity with itself.
          </p>
      </figcaption>
      </figure>
    </d-figure>

    <p>Given their usefulness for detecting continuity and discontinuity, by far the primary downstream contribution of high-low frequency detectors is to <i>boundary detectors</i>. Of the top 20 neurons in <code>mixed3b</code> with the highest L2-norm of weights across all high-low frequency detectors, eight of those 20 neurons participate in boundary detection of some sort: <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_double_boundary">double boundary detectors</a>, <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary_misc">miscellaneous boundary detectors</a>, and especially <a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">object boundary detectors</a>. </p>

    <h4>Role in object boundary detection</h4>

    <p><a href="https://distill.pub/2020/circuits/early-vision/#group_mixed3b_boundary">Object boundary detectors</a></i> are neurons which detect boundaries between objects, whether that means the boundary between one object and another or the transition from foreground to background. These detectors are not the same as edge detectors or curve detectors: although they are sensitive to edges (indeed, some of their strongest weights are from lower-level edge detectors!), object boundary detectors are also sensitive to other indicators such as color contrast or high-low frequency, combining together multiple such cues from lower layers.</p>


    <figure style='image-rendering: pixelated;'>
      <img src="images/Boundary.svg" />
    </figure>

    <p> High-low frequency detectors contribute to these object boundary detectors by providing one type of cue that an object has ended and something else has begun. An additional three examples of object boundary detectors are given below; notice how both the shape of the weights and the orientation of the high-low frequency detector each contribute to the object boundary detector's shape.</p>

    <d-figure>
      <figure>
        <img src="diagrams/downstream-boundary-detectors.svg" style="margin-bottom: 20px;"/>
      <figcaption>
        <p>Four examples of object boundary detectors that high-low frequency detectors contribute to: <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_345.html"><code>mixed3b</code> 345</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_376.html"><code>mixed3b</code> 376</a>, <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_368.html"><code>mixed3b</code> 368</a>, and <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_151.html"><code>mixed3b</code> 151</a>.
      </figcaption>
      </figure>
    </d-figure>

    <p>Object boundary detectors are sensitive to the rotation of the high-low frequency detectors that activate them. Consider how <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3b_345.html"><code>mixed3b</code> 345</a>, the top example above, has its strongest activations from high-low frequency detectors that detect change across a vertically-oriented line.</p>

    <p>However, much the other examples we've seen, object boundary detectors are also invariant to the &ldquo;sidedness&rdquo; of the high-low frequency detectors that activate them. Among the high-low frequency detectors that activate <code>mixed3b</code> 345, the high-frequency side may either be on the left (as with <code>mixed3a</code> <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_110.html">110</a>) or on the right (as with <code>mixed3a</code> <a href="https://storage.googleapis.com/inceptionv1-weight-explorer/mixed3a_112.html">112</a>); in each case, the weights are approximately the same.</p>

    <!-- <p class="todo">Show cosine similarity of forward weights as compared to incoming weights; expect -1 incoming; 1 outgoing. Can put this in a table?</p> -->

    <h3>In mixed4a</h3>

    <p>
      High-low frequency detectors ultimately play a role in detecting more sophisticated object shapes in <code>mixed4a</code> and beyond by contributing to the detection of boundaries and contiguity.
    </p>

    <figure id="3.2-downstream-nmf">
      <img src="diagrams/3.2-downstream-nmf/placeholder.png" />
      <figcaption class="figcaption">
        <p>8 neurons from <code>mixed4a</code> (top row) with the highest L2 norm of weights on the given 9 boundary detectors (left column) from <code>mixed3a</code>.</p>
        <p>Where a neuron is <span class="legend-label support">activated</span> by a high-low frequency detector in some direction, the object typically has a <i>boundary</i> along the same line.</p>
        <p>Where a neuron is <span
        class="legend-label inhibit">inhibited</span> by a high-low frequency detector in some direction, the object typically has <i>contiguity</i> orthogonal to the line of high-low frequency change.</p>
      </figcaption>
    </figure>

    <p>We have investigated the function of high-low frequency detectors, their construction from neurons in <code>conv2d2</code>, and their applications in <code>mixed3b</code> and beyond.

    So far, however, the scope of our investigation has been limited to InceptionV1.

    How common are high-low frequency detectors in convolutional neural networks generally?</p>

    <!--Universality-->
    <h2>Universality</h2>

    <h4>High-Low Frequency Detectors in Other Networks</h4>

    <p>It's always good to ask if what you see is the rule or an interesting exception&mdash;and high-low frequency detectors seem to be the rule.
          High-low frequency detectors similar to ones in InceptionV1 can be found in a variety of architectures. Below, we show four columns of high-low frequency detectors that we've found in AlexNet, InceptionV4, and ResnetV2-50, and compare them to their most similar counterpart from InceptionV1.
      </p>


    <%= require('./diagrams/similar-directions.ejs')() %>

      <p>Notice that these detectors are found at very similar depths within the different networks, between 29% and 33% network depth!</p>

    <p>Having located candidate high-low frequency detectors in these three other networks, we can also ask ourselves whether the same conclusions hold about how these detectors are built up from underlying neurons.</p>

    <h4>HF-factor and LF-factor in Other networks</h4>

    <p>
      As with InceptionV1, we perform NMF on the weights of the high-low frequency detectors in each network.
    </p>

    <%= require('./diagrams/universality.ejs')() %>


    <p>As we found before, the feature visualizations of the two factors reveal one clear HF-factor and one clear LF-factor. Furthermore, the weights of the two factors are again very close to symmetric.</p>
    <p>While the small details&mdash;which angles each network's high-low frequency detectors respond to, which colors appear in each network's feature visualizations&mdash;may vary, the overall thrust of the story is still the same: high-low frequency detectors are built up by placing high frequency detection and low frequency detection in a specific spatial arrangement.</p>

    <h2>Conclusion</h2>

    <p>As we've seen, high-low frequency detectors are an important part of later, higher-level features, are built up from comprehensible lower-level parts, and are common across multiple network architectures.</p>

    <p>We wonder whether their existence isn't so unnatural after all: as seen previously in <a href="#dataset-examples">the dataset examples</a>, a high-low frequency change can indicate the border between an in-focus foreground object and its more blurry background (or between an in-focus background and the more blurry foreground, or even between two objects at different focuses).</a> Changes such as these are something of a natural concept: the aesthetic quality imparted by the blurriness of an out-of-focus region of an image is known as <a href="https://en.wikipedia.org/wiki/Bokeh"><i>bokeh</i></a> in photography, and getting it right as a depth cue is one of the notable challenges explored by developers of immersive VR.</p>

    <p>Depth perception is simply one of the challenges that's endemic to vision, and frequency change is one cue that a vision system can use for getting it right. Just as neuroscientists have already uncovered and characterized curve detectors, the authors therefore wonder whether high-low frequency detectors might also be lurking in that original neural network vision system, the animal visual cortex.</p>




  </d-article>

  <d-appendix>
    <h3>Author Contributions</h3>
    <p>
      <b>Research:</b> Alex developed ...
    </p>

    <p>
      <b>Writing & Diagrams:</b> The text was initially drafted by...
    </p>

    <!--Potential Limitations-->
    <!-- <h3>Limitations</h3>
    <h4>How bad is the linearization?</h4>
    <p class="todo">Maybe we can measure and put up some <em>summary statistics</em> on how bad our linear approximation
      really is.
      Maybe in terms of % variance of resulting activation? -> colah if there's sth more principled.</p>

    <figure id="5.1-linearization-badness">
      <img src="diagrams/5.1-linearization-badness/placeholder.jpg" />
      <figcaption class="figcaption">
        Maybe show % of variance explained over distance between layers or sth; if so likely move to appendix.
      </figcaption>
    </figure> -->


    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>

</body>
